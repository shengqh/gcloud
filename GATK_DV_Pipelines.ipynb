{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "c-H3xOHUVw_A"
   },
   "source": [
    "![](https://www.broadinstitute.org/files/images/GoogleGenomics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "0woV2WOwVw_B"
   },
   "source": [
    "## GenX secondary analysis on GCP - GATK4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](https://software.broadinstitute.org/gatk/img/pipeline_overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 72,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1519137548517,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "VNtWHRi9Vw_D",
    "outputId": "f7322c43-0c73-454e-ffc1-09a6ac7c8759"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/gatk-workflows/broad-prod-wgs-germline-snps-indels.git  #to run it on GCP\n",
    "git clone https://github.com/openwdl/wdl.git open_wdl               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## In order to run the GATK pipeline, we need our pipeline yaml file, wdl file, inputs, and options.\n",
    "https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prod-wgs-germline-snps-indels\n",
      "### Purpose : \n",
      "Workflows used in production at Broad for germline short variant discovery in WGS data. \n",
      "\n",
      "### PairedSingleSampleWF :\n",
      "This WDL pipeline implements data pre-processing and initial variant calling (GVCF\n",
      "generation) according to the GATK Best Practices (June 2016) for germline SNP and\n",
      "Indel discovery in human whole-genome sequencing (WGS) data.\n",
      "\n",
      "#### Requirements/expectations\n",
      "- Human whole-genome pair-end sequencing data in unmapped BAM (uBAM) format\n",
      "- One or more read groups, one per uBAM file, all belonging to a single sample (SM)\n",
      "- Input uBAM files must additionally comply with the following requirements:\n",
      "- - filenames all have the same suffix (we use \".unmapped.bam\")\n",
      "- - files must pass validation by ValidateSamFile\n",
      "- - reads are provided in query-sorted order\n",
      "- - all reads must have an RG tag\n",
      "- Reference genome must be Hg38 with ALT contigs\n",
      "#### Outputs \n",
      "- Cram, cram index, and cram md5 \n",
      "- GVCF and its gvcf index \n",
      "- BQSR Report\n",
      "- Several Summary Metrics \n",
      "\n",
      "### joint-discovery-gatk :\n",
      "The second WDL implements the joint discovery and VQSR \n",
      "filtering portion of the GATK Best Practices (June 2016) for germline SNP and Indel \n",
      "discovery in human whole-genome sequencing (WGS) and exome sequencing data.\n",
      "\n",
      "#### Requirements/expectations\n",
      "- One or more GVCFs produced by HaplotypeCaller in GVCF mode.\n",
      "- Bare minimum 1 WGS sample or 30 Exome samples. Gene panels are not supported.\n",
      "#### Outputs \n",
      "- VCF  and its vcf index\n",
      " *Note: The gvcf is filtered using variant quality score recalibration  \n",
      "  (VQSR) with genotypes for all samples present in the input VCF. All sites that  \n",
      "  are present in the input VCF are retained; filtered sites are annotated as such  \n",
      "  in the FILTER field.*\n",
      "- Summary Metrics\n",
      "\n",
      "### Software version requirements :\n",
      "- GATK 4.beta.3 or later \n",
      "- Picard 2.x\n",
      "- Samtools (see gotc docker)\n",
      "- Python 2.7\n",
      "\n",
      "Cromwell version support \n",
      "- Successfully tested on v29\n",
      "- Does not work on versions < v23 due to output syntax\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat broad-prod-wgs-germline-snps-indels/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "VKJTekOvVw_K"
   },
   "source": [
    "#### Let's see what the input (reference and test genomes) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 1110,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1174,
     "status": "ok",
     "timestamp": 1519137563104,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "Z1_LFHHcVw_L",
    "outputId": "95a5732b-ee86-46c4-d2ff-bfc74cde80cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"##_COMMENT1\": \"Take note of the .64 extensions on the reference files, issues between 32 and 64 bit OS\",\n",
      "\n",
      "  \"##_COMMENT2\": \"SAMPLE NAME AND UNMAPPED BAMS - read the README to find other examples.\",\n",
      "  \"PairedEndSingleSampleWorkflow.sample_name\": \"NA12878\",\n",
      "  \"PairedEndSingleSampleWorkflow.base_file_name\": \"NA12878\",\n",
      "  \"PairedEndSingleSampleWorkflow.flowcell_unmapped_bams\": [\"gs://broad-public-datasets/NA12878_downsampled_for_testing/unmapped/H06HDADXX130110.1.ATCACGAT.20k_reads.bam\",\n",
      "    \"gs://broad-public-datasets/NA12878_downsampled_for_testing/unmapped/H06HDADXX130110.2.ATCACGAT.20k_reads.bam\",\n",
      "    \"gs://broad-public-datasets/NA12878_downsampled_for_testing/unmapped/H06JUADXX130110.1.ATCACGAT.20k_reads.bam\"],\n",
      "  \"PairedEndSingleSampleWorkflow.final_gvcf_base_name\": \"NA12878\",\n",
      "  \"PairedEndSingleSampleWorkflow.unmapped_bam_suffix\": \".bam\",\n",
      "\n",
      "  \"##_COMMENT3\": \"REFERENCES\",\n",
      "  \"PairedEndSingleSampleWorkflow.fingerprint_genotypes_file\": \"gs://dsde-data-na12878-public/NA12878.hg38.reference.fingerprint.vcf\",\n",
      "  \"PairedEndSingleSampleWorkflow.contamination_sites_ud\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.contam.UD\",\n",
      "  \"PairedEndSingleSampleWorkflow.contamination_sites_bed\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.contam.bed\",\n",
      "  \"PairedEndSingleSampleWorkflow.contamination_sites_mu\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.contam.mu\",\n",
      "  \"PairedEndSingleSampleWorkflow.scattered_calling_intervals\": [\"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0001_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0002_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0003_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0004_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0005_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0006_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0007_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0008_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0009_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0010_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0011_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0012_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0013_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0014_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0015_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0016_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0017_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0019_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0020_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0021_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0022_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0023_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0024_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0025_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0026_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0027_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0028_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0029_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0030_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0031_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0032_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0033_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0034_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0035_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0036_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0037_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0038_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0039_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0040_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0041_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0042_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0043_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0044_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0045_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0046_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0047_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0048_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0049_of_50/scattered.interval_list\", \"gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0050_of_50/scattered.interval_list\"],\n",
      "  \"PairedEndSingleSampleWorkflow.wgs_calling_interval_list\": \"gs://broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_dict\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_fasta\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_fasta_index\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_alt\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.alt\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_sa\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.sa\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_amb\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.amb\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_bwt\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.bwt\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_ann\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.ann\",\n",
      "  \"PairedEndSingleSampleWorkflow.ref_pac\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.pac\",\n",
      "  \"PairedEndSingleSampleWorkflow.known_indels_sites_VCFs\": [\n",
      "    \"gs://broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\",\n",
      "    \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz\"\n",
      "  ],\n",
      "  \"PairedEndSingleSampleWorkflow.known_indels_sites_indices\": [\n",
      "    \"gs://broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi\",\n",
      "    \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi\"\n",
      "  ],\n",
      "  \"PairedEndSingleSampleWorkflow.dbSNP_vcf\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\",\n",
      "  \"PairedEndSingleSampleWorkflow.dbSNP_vcf_index\": \"gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx\",\n",
      "  \"PairedEndSingleSampleWorkflow.wgs_coverage_interval_list\": \"gs://broad-references/hg38/v0/wgs_coverage_regions.hg38.interval_list\",\n",
      "  \"PairedEndSingleSampleWorkflow.wgs_evaluation_interval_list\": \"gs://broad-references/hg38/v0/wgs_evaluation_regions.hg38.interval_list\",\n",
      "\n",
      "  \"##_COMMENT4\": \"PRIVATE REFERENCES\",\n",
      "  \"##PairedEndSingleSampleWorkflow.haplotype_database_file\": \"gs://gatk-aas-test-data/small/empty.vcf\",\n",
      "\n",
      "  \"##_COMMENT5\": \"DISK SIZES + MISC\",\n",
      "  \"PairedEndSingleSampleWorkflow.flowcell_small_disk\": 100,\n",
      "  \"PairedEndSingleSampleWorkflow.flowcell_medium_disk\": 200,\n",
      "  \"PairedEndSingleSampleWorkflow.agg_small_disk\": 200,\n",
      "  \"PairedEndSingleSampleWorkflow.agg_medium_disk\": 300,\n",
      "  \"PairedEndSingleSampleWorkflow.agg_large_disk\": 400,\n",
      "  \"PairedEndSingleSampleWorkflow.preemptible_tries\": 3,\n",
      "  \"PairedEndSingleSampleWorkflow.agg_preemptible_tries\": 3,\n",
      "\n",
      "  \"##_COMMENT6\": \"MISC\",\n",
      "  \"PairedEndSingleSampleWorkflow.break_bands_at_multiples_of\": 1000000,\n",
      "  \"PairedEndSingleSampleWorkflow.haplotype_scatter_count\": 50 \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat broad-prod-wgs-germline-snps-indels/PairedEndSingleSampleWf.hg38.inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "VUhO5XVmVw_O"
   },
   "source": [
    "#### Lets take a look at the  yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 461,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1453,
     "status": "ok",
     "timestamp": 1519140444530,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "mqv-Bps0Vw_P",
    "outputId": "9ec349e5-b1d0-4241-a7c7-ed727ab51c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: WDL Runner\n",
      "description: Run a workflow defined by a WDL file\n",
      "\n",
      "inputParameters:\n",
      "- name: WDL\n",
      "  description: Workflow definition\n",
      "- name: WORKFLOW_INPUTS\n",
      "  description: Workflow inputs\n",
      "- name: WORKFLOW_OPTIONS\n",
      "  description: Workflow options\n",
      "\n",
      "- name: WORKSPACE\n",
      "  description: Cloud Storage path for intermediate files\n",
      "- name: OUTPUTS\n",
      "  description: Cloud Storage path for output files\n",
      "\n",
      "docker:\n",
      "  imageName: gcr.io/broad-dsde-outreach/wdl_runner:2017_10_02\n",
      "\n",
      "  cmd: >\n",
      "    /wdl_runner/wdl_runner.sh\n",
      "\n",
      "resources:\n",
      "  minimumRamGb: 3.75\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What does our WDL file look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Copyright Broad Institute, 2017\n",
      "##\n",
      "## This WDL pipeline implements data pre-processing and initial variant calling (GVCF\n",
      "## generation) according to the GATK Best Practices (June 2016) for germline SNP and\n",
      "## Indel discovery in human whole-genome sequencing (WGS) data.\n",
      "##\n",
      "## Requirements/expectations :\n",
      "## - Human whole-genome pair-end sequencing data in unmapped BAM (uBAM) format\n",
      "## - One or more read groups, one per uBAM file, all belonging to a single sample (SM)\n",
      "## - Input uBAM files must additionally comply with the following requirements:\n",
      "## - - filenames all have the same suffix (we use \".unmapped.bam\")\n",
      "## - - files must pass validation by ValidateSamFile\n",
      "## - - reads are provided in query-sorted order\n",
      "## - - all reads must have an RG tag\n",
      "## - GVCF output names must end in \".g.vcf.gz\"\n",
      "## - Reference genome must be Hg38 with ALT contigs\n",
      "##\n",
      "## Runtime parameters are optimized for Broad's Google Cloud Platform implementation.\n",
      "## For program versions, see docker containers.\n",
      "##\n",
      "## LICENSING :\n",
      "## This script is released under the WDL source code license (BSD-3) (see LICENSE in\n",
      "## https://github.com/broadinstitute/wdl). Note however that the programs it calls may\n",
      "## be subject to different licenses. Users are responsible for checking that they are\n",
      "## authorized to run all programs before running this script. Please see the docker\n",
      "## page at https://hub.docker.com/r/broadinstitute/genomes-in-the-cloud/ for detailed\n",
      "## licensing information pertaining to the included programs.\n",
      "\n",
      "# WORKFLOW DEFINITION\n",
      "workflow PairedEndSingleSampleWorkflow {\n",
      "\n",
      "  File contamination_sites_ud\n",
      "  File contamination_sites_bed\n",
      "  File contamination_sites_mu\n",
      "  File? fingerprint_genotypes_file\n",
      "  File? haplotype_database_file\n",
      "  File wgs_evaluation_interval_list\n",
      "  File wgs_coverage_interval_list\n",
      "\n",
      "  String sample_name\n",
      "  String base_file_name\n",
      "  String final_gvcf_base_name\n",
      "  Array[File] flowcell_unmapped_bams\n",
      "  String unmapped_bam_suffix\n",
      "\n",
      "  File wgs_calling_interval_list\n",
      "  Int haplotype_scatter_count\n",
      "  Int break_bands_at_multiples_of\n",
      "  Int? read_length\n",
      "\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  File ref_dict\n",
      "  File ref_alt\n",
      "  File ref_bwt\n",
      "  File ref_sa\n",
      "  File ref_amb\n",
      "  File ref_ann\n",
      "  File ref_pac\n",
      "\n",
      "  File dbSNP_vcf\n",
      "  File dbSNP_vcf_index\n",
      "  Array[File] known_indels_sites_VCFs\n",
      "  Array[File] known_indels_sites_indices\n",
      "\n",
      "  Int preemptible_tries\n",
      "  Int agg_preemptible_tries\n",
      "\n",
      "  # Optional input to increase all disk sizes in case of outlier sample with strange size behavior\n",
      "  Int? increase_disk_size\n",
      "\n",
      "  # Some tasks need wiggle room, and we also need to add a small amount of disk to prevent getting a\n",
      "  # Cromwell error from asking for 0 disk when the input is less than 1GB\n",
      "  Int additional_disk = select_first([increase_disk_size, 20])\n",
      "  # Germline single sample GVCFs shouldn't get bigger even when the input bam is bigger (after a certain size)\n",
      "  Int GVCF_disk_size = select_first([increase_disk_size, 30])\n",
      "  # Sometimes the output is larger than the input, or a task can spill to disk. In these cases we need to account for the\n",
      "  # input (1) and the output (1.5) or the input(1), the output(1), and spillage (.5).\n",
      "  Float bwa_disk_multiplier = 2.5\n",
      "  # SortSam spills to disk a lot more because we are only store 300000 records in RAM now because its faster for our data\n",
      "  # so it needs more disk space.  Also it spills to disk in an uncompressed format so we need to account for that with a\n",
      "  # larger multiplier\n",
      "  Float sort_sam_disk_multiplier = 3.25\n",
      "\n",
      "  # Mark Duplicates takes in as input readgroup bams and outputs a slightly smaller aggregated bam. Giving .25 as wiggleroom\n",
      "  Float md_disk_multiplier = 2.25\n",
      "\n",
      "  # ValidateSamFile runs out of memory in mate validation on crazy edge case data, so we want to skip the mate validation\n",
      "  # in those cases.  These values set the thresholds for what is considered outside the normal realm of \"reasonable\" data.\n",
      "  Float max_duplication_in_reasonable_sample = 0.30\n",
      "  Float max_chimerism_in_reasonable_sample = 0.15\n",
      "\n",
      "  String bwa_commandline=\"bwa mem -K 100000000 -p -v 3 -t 16 -Y $bash_ref_fasta\"\n",
      "\n",
      "  String recalibrated_bam_basename = base_file_name + \".aligned.duplicates_marked.recalibrated\"\n",
      "\n",
      "  Int compression_level = 2\n",
      "\n",
      "  # Get the version of BWA to include in the PG record in the header of the BAM produced\n",
      "  # by MergeBamAlignment.\n",
      "  call GetBwaVersion\n",
      "\n",
      "  # Get the size of the standard reference files as well as the additional reference files needed for BWA\n",
      "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fasta_index, \"GB\") + size(ref_dict, \"GB\")\n",
      "  Float bwa_ref_size = ref_size + size(ref_alt, \"GB\") + size(ref_amb, \"GB\") + size(ref_ann, \"GB\") + size(ref_bwt, \"GB\") + size(ref_pac, \"GB\") + size(ref_sa, \"GB\")\n",
      "  Float dbsnp_size = size(dbSNP_vcf, \"GB\")\n",
      "\n",
      "  # Align flowcell-level unmapped input bams in parallel\n",
      "  scatter (unmapped_bam in flowcell_unmapped_bams) {\n",
      "\n",
      "    Float unmapped_bam_size = size(unmapped_bam, \"GB\")\n",
      "\n",
      "    String sub_strip_path = \"gs://.*/\"\n",
      "    String sub_strip_unmapped = unmapped_bam_suffix + \"$\"\n",
      "    String sub_sub = sub(sub(unmapped_bam, sub_strip_path, \"\"), sub_strip_unmapped, \"\")\n",
      "\n",
      "    # QC the unmapped BAM\n",
      "    call CollectQualityYieldMetrics {\n",
      "      input:\n",
      "        input_bam = unmapped_bam,\n",
      "        metrics_filename = sub_sub + \".unmapped.quality_yield_metrics\",\n",
      "        disk_size = unmapped_bam_size + additional_disk,\n",
      "        preemptible_tries = preemptible_tries\n",
      "    }\n",
      "\n",
      "    # Map reads to reference\n",
      "    call SamToFastqAndBwaMemAndMba {\n",
      "      input:\n",
      "        input_bam = unmapped_bam,\n",
      "        bwa_commandline = bwa_commandline,\n",
      "        output_bam_basename = sub_sub + \".aligned.unsorted\",\n",
      "        ref_fasta = ref_fasta,\n",
      "        ref_fasta_index = ref_fasta_index,\n",
      "        ref_dict = ref_dict,\n",
      "        ref_alt = ref_alt,\n",
      "        ref_bwt = ref_bwt,\n",
      "        ref_amb = ref_amb,\n",
      "        ref_ann = ref_ann,\n",
      "        ref_pac = ref_pac,\n",
      "        ref_sa = ref_sa,\n",
      "        bwa_version = GetBwaVersion.version,\n",
      "        # The merged bam can be bigger than only the aligned bam,\n",
      "        # so account for the output size by multiplying the input size by 2.75.\n",
      "        disk_size = unmapped_bam_size + bwa_ref_size + (bwa_disk_multiplier * unmapped_bam_size) + additional_disk,\n",
      "        compression_level = compression_level,\n",
      "        preemptible_tries = preemptible_tries\n",
      "    }\n",
      "\n",
      "    Float mapped_bam_size = size(SamToFastqAndBwaMemAndMba.output_bam, \"GB\")\n",
      "\n",
      "    # QC the aligned but unsorted readgroup BAM\n",
      "    # no reference as the input here is unsorted, providing a reference would cause an error\n",
      "    call CollectUnsortedReadgroupBamQualityMetrics {\n",
      "      input:\n",
      "        input_bam = SamToFastqAndBwaMemAndMba.output_bam,\n",
      "        output_bam_prefix = sub_sub + \".readgroup\",\n",
      "        disk_size = mapped_bam_size + additional_disk,\n",
      "        preemptible_tries = preemptible_tries\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # Sum the read group bam sizes to approximate the aggregated bam size\n",
      "  call SumFloats {\n",
      "    input:\n",
      "      sizes = mapped_bam_size,\n",
      "      preemptible_tries = preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Aggregate aligned+merged flowcell BAM files and mark duplicates\n",
      "  # We take advantage of the tool's ability to take multiple BAM inputs and write out a single output\n",
      "  # to avoid having to spend time just merging BAM files.\n",
      "  call MarkDuplicates {\n",
      "    input:\n",
      "      input_bams = SamToFastqAndBwaMemAndMba.output_bam,\n",
      "      output_bam_basename = base_file_name + \".aligned.unsorted.duplicates_marked\",\n",
      "      metrics_filename = base_file_name + \".duplicate_metrics\",\n",
      "      # The merged bam will be smaller than the sum of the parts so we need to account for the unmerged inputs\n",
      "      # and the merged output.\n",
      "      disk_size = (md_disk_multiplier * SumFloats.total_size) + additional_disk,\n",
      "      compression_level = compression_level,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  Float agg_bam_size = size(MarkDuplicates.output_bam, \"GB\")\n",
      "\n",
      "  # Sort aggregated+deduped BAM file and fix tags\n",
      "  call SortSam as SortSampleBam {\n",
      "    input:\n",
      "      input_bam = MarkDuplicates.output_bam,\n",
      "      output_bam_basename = base_file_name + \".aligned.duplicate_marked.sorted\",\n",
      "      # This task spills to disk so we need space for the input bam, the output bam, and any spillage.\n",
      "      disk_size = (sort_sam_disk_multiplier * agg_bam_size) + additional_disk,\n",
      "      compression_level = compression_level,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  if (defined(haplotype_database_file)) {\n",
      "    # Check identity of fingerprints across readgroups\n",
      "    call CrossCheckFingerprints {\n",
      "      input:\n",
      "        input_bams = SortSampleBam.output_bam,\n",
      "        input_bam_indexes = SortSampleBam.output_bam_index,\n",
      "        haplotype_database_file = haplotype_database_file,\n",
      "        metrics_filename = base_file_name + \".crosscheck\",\n",
      "        disk_size = agg_bam_size + additional_disk,\n",
      "        preemptible_tries = agg_preemptible_tries\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # Create list of sequences for scatter-gather parallelization\n",
      "  call CreateSequenceGroupingTSV {\n",
      "    input:\n",
      "      ref_dict = ref_dict,\n",
      "      preemptible_tries = preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Estimate level of cross-sample contamination\n",
      "  call CheckContamination {\n",
      "    input:\n",
      "      input_bam = SortSampleBam.output_bam,\n",
      "      input_bam_index = SortSampleBam.output_bam_index,\n",
      "      contamination_sites_ud = contamination_sites_ud,\n",
      "      contamination_sites_bed = contamination_sites_bed,\n",
      "      contamination_sites_mu = contamination_sites_mu,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      output_prefix = base_file_name + \".preBqsr\",\n",
      "      disk_size = agg_bam_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries,\n",
      "      contamination_underestimation_factor = 0.75\n",
      "  }\n",
      "\n",
      "  # We need disk to localize the sharded input and output due to the scatter for BQSR.\n",
      "  # If we take the number we are scattering by and reduce by 3 we will have enough disk space\n",
      "  # to account for the fact that the data is not split evenly.\n",
      "  Int num_of_bqsr_scatters = length(CreateSequenceGroupingTSV.sequence_grouping)\n",
      "  Int potential_bqsr_divisor = num_of_bqsr_scatters - 10\n",
      "  Int bqsr_divisor = if potential_bqsr_divisor > 1 then potential_bqsr_divisor else 1\n",
      "\n",
      "  # Perform Base Quality Score Recalibration (BQSR) on the sorted BAM in parallel\n",
      "  scatter (subgroup in CreateSequenceGroupingTSV.sequence_grouping) {\n",
      "    # Generate the recalibration model by interval\n",
      "    call BaseRecalibrator {\n",
      "      input:\n",
      "        input_bam = SortSampleBam.output_bam,\n",
      "        recalibration_report_filename = base_file_name + \".recal_data.csv\",\n",
      "        sequence_group_interval = subgroup,\n",
      "        dbSNP_vcf = dbSNP_vcf,\n",
      "        dbSNP_vcf_index = dbSNP_vcf_index,\n",
      "        known_indels_sites_VCFs = known_indels_sites_VCFs,\n",
      "        known_indels_sites_indices = known_indels_sites_indices,\n",
      "        ref_dict = ref_dict,\n",
      "        ref_fasta = ref_fasta,\n",
      "        ref_fasta_index = ref_fasta_index,\n",
      "        # We need disk to localize the sharded bam due to the scatter.\n",
      "        disk_size = (agg_bam_size / bqsr_divisor) + ref_size + dbsnp_size + additional_disk,\n",
      "        preemptible_tries = agg_preemptible_tries\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # Merge the recalibration reports resulting from by-interval recalibration\n",
      "  # The reports are always the same size\n",
      "  call GatherBqsrReports {\n",
      "    input:\n",
      "      input_bqsr_reports = BaseRecalibrator.recalibration_report,\n",
      "      output_report_filename = base_file_name + \".recal_data.csv\",\n",
      "      disk_size = additional_disk,\n",
      "      preemptible_tries = preemptible_tries\n",
      "  }\n",
      "\n",
      "  scatter (subgroup in CreateSequenceGroupingTSV.sequence_grouping_with_unmapped) {\n",
      "    # Apply the recalibration model by interval\n",
      "    call ApplyBQSR {\n",
      "      input:\n",
      "        input_bam = SortSampleBam.output_bam,\n",
      "        output_bam_basename = recalibrated_bam_basename,\n",
      "        recalibration_report = GatherBqsrReports.output_bqsr_report,\n",
      "        sequence_group_interval = subgroup,\n",
      "        ref_dict = ref_dict,\n",
      "        ref_fasta = ref_fasta,\n",
      "        ref_fasta_index = ref_fasta_index,\n",
      "        # We need disk to localize the sharded bam and the sharded output due to the scatter.\n",
      "        disk_size = ((agg_bam_size * 3) / bqsr_divisor) + ref_size + additional_disk,\n",
      "        compression_level = compression_level,\n",
      "        preemptible_tries = agg_preemptible_tries\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # Merge the recalibrated BAM files resulting from by-interval recalibration\n",
      "  call GatherBamFiles {\n",
      "    input:\n",
      "      input_bams = ApplyBQSR.recalibrated_bam,\n",
      "      output_bam_basename = base_file_name,\n",
      "      # Multiply the input bam size by two to account for the input and output\n",
      "      disk_size = (2 * agg_bam_size) + additional_disk,\n",
      "      compression_level = compression_level,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  #BQSR bins the qualities which makes a significantly smaller bam\n",
      "  Float binned_qual_bam_size = size(GatherBamFiles.output_bam, \"GB\")\n",
      "\n",
      "  # QC the final BAM (consolidated after scattered BQSR)\n",
      "  call CollectReadgroupBamQualityMetrics {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      input_bam_index = GatherBamFiles.output_bam_index,\n",
      "      output_bam_prefix = base_file_name + \".readgroup\",\n",
      "      ref_dict = ref_dict,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      disk_size = binned_qual_bam_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # QC the final BAM some more (no such thing as too much QC)\n",
      "  call CollectAggregationMetrics {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      input_bam_index = GatherBamFiles.output_bam_index,\n",
      "      output_bam_prefix = base_file_name,\n",
      "      ref_dict = ref_dict,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      disk_size = binned_qual_bam_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  if (defined(haplotype_database_file) && defined(fingerprint_genotypes_file)) {\n",
      "    # Check the sample BAM fingerprint against the sample array\n",
      "    call CheckFingerprint {\n",
      "      input:\n",
      "        input_bam = GatherBamFiles.output_bam,\n",
      "        input_bam_index = GatherBamFiles.output_bam_index,\n",
      "        haplotype_database_file = haplotype_database_file,\n",
      "        genotypes = fingerprint_genotypes_file,\n",
      "        output_basename = base_file_name,\n",
      "        sample = sample_name,\n",
      "        disk_size = binned_qual_bam_size + additional_disk,\n",
      "        preemptible_tries = agg_preemptible_tries\n",
      "    }\n",
      "  }\n",
      "\n",
      "  # QC the sample WGS metrics (stringent thresholds)\n",
      "  call CollectWgsMetrics {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      input_bam_index = GatherBamFiles.output_bam_index,\n",
      "      metrics_filename = base_file_name + \".wgs_metrics\",\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      wgs_coverage_interval_list = wgs_coverage_interval_list,\n",
      "      read_length = read_length,\n",
      "      disk_size = binned_qual_bam_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # QC the sample raw WGS metrics (common thresholds)\n",
      "  call CollectRawWgsMetrics {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      input_bam_index = GatherBamFiles.output_bam_index,\n",
      "      metrics_filename = base_file_name + \".raw_wgs_metrics\",\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      wgs_coverage_interval_list = wgs_coverage_interval_list,\n",
      "      read_length = read_length,\n",
      "      disk_size = binned_qual_bam_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Generate a checksum per readgroup in the final BAM\n",
      "  call CalculateReadGroupChecksum {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      input_bam_index = GatherBamFiles.output_bam_index,\n",
      "      read_group_md5_filename = recalibrated_bam_basename + \".bam.read_group_md5\",\n",
      "      disk_size = binned_qual_bam_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Convert the final merged recalibrated BAM file to CRAM format\n",
      "  call ConvertToCram {\n",
      "    input:\n",
      "      input_bam = GatherBamFiles.output_bam,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      output_basename = base_file_name,\n",
      "      disk_size = (2 * binned_qual_bam_size) + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  Float cram_size = size(ConvertToCram.output_cram, \"GB\")\n",
      "\n",
      "  # Check whether the data has massively high duplication or chimerism rates\n",
      "  call CheckPreValidation {\n",
      "    input:\n",
      "      duplication_metrics = MarkDuplicates.duplicate_metrics,\n",
      "      chimerism_metrics = CollectAggregationMetrics.alignment_summary_metrics,\n",
      "      max_duplication_in_reasonable_sample = max_duplication_in_reasonable_sample,\n",
      "      max_chimerism_in_reasonable_sample = max_chimerism_in_reasonable_sample,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      " }\n",
      "\n",
      "  # Validate the CRAM file\n",
      "  call ValidateSamFile as ValidateCram {\n",
      "    input:\n",
      "      input_bam = ConvertToCram.output_cram,\n",
      "      input_bam_index = ConvertToCram.output_cram_index,\n",
      "      report_filename = base_file_name + \".cram.validation_report\",\n",
      "      ref_dict = ref_dict,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      ignore = [\"MISSING_TAG_NM\"],\n",
      "      max_output = 1000000000,\n",
      "      is_outlier_data = CheckPreValidation.is_outlier_data,\n",
      "      disk_size = cram_size + ref_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Break the calling interval_list into sub-intervals\n",
      "  # Perform variant calling on the sub-intervals, and then gather the results\n",
      "  call ScatterIntervalList {\n",
      "    input:\n",
      "      interval_list = wgs_calling_interval_list,\n",
      "      scatter_count = haplotype_scatter_count,\n",
      "      break_bands_at_multiples_of = break_bands_at_multiples_of\n",
      "  }\n",
      "\n",
      "  # We need disk to localize the sharded input and output due to the scatter for HaplotypeCaller.\n",
      "  # If we take the number we are scattering by and reduce by 20 we will have enough disk space\n",
      "  # to account for the fact that the data is quite uneven across the shards.\n",
      "  Int potential_hc_divisor = ScatterIntervalList.interval_count - 20\n",
      "  Int hc_divisor = if potential_hc_divisor > 1 then potential_hc_divisor else 1\n",
      "\n",
      "  # Call variants in parallel over WGS calling intervals\n",
      "  scatter (index in range(ScatterIntervalList.interval_count)) {\n",
      "    # Generate GVCF by interval\n",
      "    call HaplotypeCaller {\n",
      "      input:\n",
      "        contamination = CheckContamination.contamination,\n",
      "        input_bam = GatherBamFiles.output_bam,\n",
      "        interval_list = ScatterIntervalList.out[index],\n",
      "        gvcf_basename = base_file_name,\n",
      "        ref_dict = ref_dict,\n",
      "        ref_fasta = ref_fasta,\n",
      "        ref_fasta_index = ref_fasta_index,\n",
      "        # Divide the total output GVCF size and the input bam size to account for the smaller scattered input and output.\n",
      "        disk_size = ((binned_qual_bam_size + GVCF_disk_size) / hc_divisor) + ref_size + additional_disk,\n",
      "        preemptible_tries = agg_preemptible_tries\n",
      "     }\n",
      "  }\n",
      "\n",
      "  # Combine by-interval GVCFs into a single sample GVCF file\n",
      "  call MergeVCFs {\n",
      "    input:\n",
      "      input_vcfs = HaplotypeCaller.output_gvcf,\n",
      "      input_vcfs_indexes = HaplotypeCaller.output_gvcf_index,\n",
      "      output_vcf_name = final_gvcf_base_name + \".g.vcf.gz\",\n",
      "      disk_size = GVCF_disk_size,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  Float gvcf_size = size(MergeVCFs.output_vcf, \"GB\")\n",
      "\n",
      "  # Validate the GVCF output of HaplotypeCaller\n",
      "  call ValidateGVCF {\n",
      "    input:\n",
      "      input_vcf = MergeVCFs.output_vcf,\n",
      "      input_vcf_index = MergeVCFs.output_vcf_index,\n",
      "      dbSNP_vcf = dbSNP_vcf,\n",
      "      dbSNP_vcf_index = dbSNP_vcf_index,\n",
      "      ref_fasta = ref_fasta,\n",
      "      ref_fasta_index = ref_fasta_index,\n",
      "      ref_dict = ref_dict,\n",
      "      wgs_calling_interval_list = wgs_calling_interval_list,\n",
      "      disk_size = gvcf_size + ref_size + dbsnp_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # QC the GVCF\n",
      "  call CollectGvcfCallingMetrics {\n",
      "    input:\n",
      "      input_vcf = MergeVCFs.output_vcf,\n",
      "      input_vcf_index = MergeVCFs.output_vcf_index,\n",
      "      metrics_basename = final_gvcf_base_name,\n",
      "      dbSNP_vcf = dbSNP_vcf,\n",
      "      dbSNP_vcf_index = dbSNP_vcf_index,\n",
      "      ref_dict = ref_dict,\n",
      "      wgs_evaluation_interval_list = wgs_evaluation_interval_list,\n",
      "      disk_size = gvcf_size + dbsnp_size + additional_disk,\n",
      "      preemptible_tries = agg_preemptible_tries\n",
      "  }\n",
      "\n",
      "  # Outputs that will be retained when execution is complete\n",
      "  output {\n",
      "    Array[File] quality_yield_metrics = CollectQualityYieldMetrics.metrics\n",
      "\n",
      "    Array[File] unsorted_read_group_base_distribution_by_cycle_pdf = CollectUnsortedReadgroupBamQualityMetrics.base_distribution_by_cycle_pdf\n",
      "    Array[File] unsorted_read_group_base_distribution_by_cycle_metrics = CollectUnsortedReadgroupBamQualityMetrics.base_distribution_by_cycle_metrics\n",
      "    Array[File] unsorted_read_group_insert_size_histogram_pdf = CollectUnsortedReadgroupBamQualityMetrics.insert_size_histogram_pdf\n",
      "    Array[File] unsorted_read_group_insert_size_metrics = CollectUnsortedReadgroupBamQualityMetrics.insert_size_metrics\n",
      "    Array[File] unsorted_read_group_quality_by_cycle_pdf = CollectUnsortedReadgroupBamQualityMetrics.quality_by_cycle_pdf\n",
      "    Array[File] unsorted_read_group_quality_by_cycle_metrics = CollectUnsortedReadgroupBamQualityMetrics.quality_by_cycle_metrics\n",
      "    Array[File] unsorted_read_group_quality_distribution_pdf = CollectUnsortedReadgroupBamQualityMetrics.quality_distribution_pdf\n",
      "    Array[File] unsorted_read_group_quality_distribution_metrics = CollectUnsortedReadgroupBamQualityMetrics.quality_distribution_metrics\n",
      "\n",
      "    File read_group_alignment_summary_metrics = CollectReadgroupBamQualityMetrics.alignment_summary_metrics\n",
      "    File read_group_gc_bias_detail_metrics = CollectReadgroupBamQualityMetrics.gc_bias_detail_metrics\n",
      "    File read_group_gc_bias_pdf = CollectReadgroupBamQualityMetrics.gc_bias_pdf\n",
      "    File read_group_gc_bias_summary_metrics = CollectReadgroupBamQualityMetrics.gc_bias_summary_metrics\n",
      "\n",
      "    File? cross_check_fingerprints_metrics = CrossCheckFingerprints.metrics\n",
      "\n",
      "    File selfSM = CheckContamination.selfSM\n",
      "    Float contamination = CheckContamination.contamination\n",
      "\n",
      "    File calculate_read_group_checksum_md5 = CalculateReadGroupChecksum.md5_file\n",
      "\n",
      "    File agg_alignment_summary_metrics = CollectAggregationMetrics.alignment_summary_metrics\n",
      "    File agg_bait_bias_detail_metrics = CollectAggregationMetrics.bait_bias_detail_metrics\n",
      "    File agg_bait_bias_summary_metrics = CollectAggregationMetrics.bait_bias_summary_metrics\n",
      "    File agg_gc_bias_detail_metrics = CollectAggregationMetrics.gc_bias_detail_metrics\n",
      "    File agg_gc_bias_pdf = CollectAggregationMetrics.gc_bias_pdf\n",
      "    File agg_gc_bias_summary_metrics = CollectAggregationMetrics.gc_bias_summary_metrics\n",
      "    File agg_insert_size_histogram_pdf = CollectAggregationMetrics.insert_size_histogram_pdf\n",
      "    File agg_insert_size_metrics = CollectAggregationMetrics.insert_size_metrics\n",
      "    File agg_pre_adapter_detail_metrics = CollectAggregationMetrics.pre_adapter_detail_metrics\n",
      "    File agg_pre_adapter_summary_metrics = CollectAggregationMetrics.pre_adapter_summary_metrics\n",
      "    File agg_quality_distribution_pdf = CollectAggregationMetrics.quality_distribution_pdf\n",
      "    File agg_quality_distribution_metrics = CollectAggregationMetrics.quality_distribution_metrics\n",
      "\n",
      "    File? fingerprint_summary_metrics = CheckFingerprint.summary_metrics\n",
      "    File? fingerprint_detail_metrics = CheckFingerprint.detail_metrics\n",
      "\n",
      "    File wgs_metrics = CollectWgsMetrics.metrics\n",
      "    File raw_wgs_metrics = CollectRawWgsMetrics.metrics\n",
      "\n",
      "    File gvcf_summary_metrics = CollectGvcfCallingMetrics.summary_metrics\n",
      "    File gvcf_detail_metrics = CollectGvcfCallingMetrics.detail_metrics\n",
      "\n",
      "    File duplicate_metrics = MarkDuplicates.duplicate_metrics\n",
      "    File output_bqsr_reports = GatherBqsrReports.output_bqsr_report\n",
      "\n",
      "    File output_cram = ConvertToCram.output_cram\n",
      "    File output_cram_index = ConvertToCram.output_cram_index\n",
      "    File output_cram_md5 = ConvertToCram.output_cram_md5\n",
      "\n",
      "    File validate_cram_file_report = ValidateCram.report\n",
      "\n",
      "    File output_vcf = MergeVCFs.output_vcf\n",
      "    File output_vcf_index = MergeVCFs.output_vcf_index\n",
      "  }\n",
      "}\n",
      "\n",
      "# TASK DEFINITIONS\n",
      "\n",
      "# Collect sequencing yield quality metrics\n",
      "task CollectQualityYieldMetrics {\n",
      "  File input_bam\n",
      "  String metrics_filename\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectQualityYieldMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OQ=true \\\n",
      "      OUTPUT=${metrics_filename}\n",
      "  }\n",
      "  runtime {\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    memory: \"3 GB\"\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File metrics = \"${metrics_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Get version of BWA\n",
      "task GetBwaVersion {\n",
      "  command {\n",
      "    # not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because\n",
      "    # the sed may also fail with that error and that is something we actually want to fail on.\n",
      "    /usr/gitc/bwa 2>&1 | \\\n",
      "    grep -e '^Version' | \\\n",
      "    sed 's/Version: //'\n",
      "  }\n",
      "  runtime {\n",
      "    memory: \"1 GB\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    String version = read_string(stdout())\n",
      "  }\n",
      "}\n",
      "\n",
      "# Read unmapped BAM, convert on-the-fly to FASTQ and stream to BWA MEM for alignment, then stream to MergeBamAlignment\n",
      "task SamToFastqAndBwaMemAndMba {\n",
      "  File input_bam\n",
      "  String bwa_commandline\n",
      "  String bwa_version\n",
      "  String output_bam_basename\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  File ref_dict\n",
      "\n",
      "  # This is the .alt file from bwa-kit (https://github.com/lh3/bwa/tree/master/bwakit),\n",
      "  # listing the reference contigs that are \"alternative\".\n",
      "  File ref_alt\n",
      "\n",
      "  File ref_amb\n",
      "  File ref_ann\n",
      "  File ref_bwt\n",
      "  File ref_pac\n",
      "  File ref_sa\n",
      "  Float disk_size\n",
      "  Int compression_level\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "    set -o pipefail\n",
      "    set -e\n",
      "\n",
      "    # set the bash variable needed for the command-line\n",
      "    bash_ref_fasta=${ref_fasta}\n",
      "    # if ref_alt has data in it,\n",
      "    if [ -s ${ref_alt} ]; then\n",
      "      java -Xms5000m -jar /usr/gitc/picard.jar \\\n",
      "        SamToFastq \\\n",
      "        INPUT=${input_bam} \\\n",
      "        FASTQ=/dev/stdout \\\n",
      "        INTERLEAVE=true \\\n",
      "        NON_PF=true | \\\n",
      "      /usr/gitc/${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) | \\\n",
      "      java -Dsamjdk.compression_level=${compression_level} -Xms3000m -jar /usr/gitc/picard.jar \\\n",
      "        MergeBamAlignment \\\n",
      "        VALIDATION_STRINGENCY=SILENT \\\n",
      "        EXPECTED_ORIENTATIONS=FR \\\n",
      "        ATTRIBUTES_TO_RETAIN=X0 \\\n",
      "        ATTRIBUTES_TO_REMOVE=NM \\\n",
      "        ATTRIBUTES_TO_REMOVE=MD \\\n",
      "        ALIGNED_BAM=/dev/stdin \\\n",
      "        UNMAPPED_BAM=${input_bam} \\\n",
      "        OUTPUT=${output_bam_basename}.bam \\\n",
      "        REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "        PAIRED_RUN=true \\\n",
      "        SORT_ORDER=\"unsorted\" \\\n",
      "        IS_BISULFITE_SEQUENCE=false \\\n",
      "        ALIGNED_READS_ONLY=false \\\n",
      "        CLIP_ADAPTERS=false \\\n",
      "        MAX_RECORDS_IN_RAM=2000000 \\\n",
      "        ADD_MATE_CIGAR=true \\\n",
      "        MAX_INSERTIONS_OR_DELETIONS=-1 \\\n",
      "        PRIMARY_ALIGNMENT_STRATEGY=MostDistant \\\n",
      "        PROGRAM_RECORD_ID=\"bwamem\" \\\n",
      "        PROGRAM_GROUP_VERSION=\"${bwa_version}\" \\\n",
      "        PROGRAM_GROUP_COMMAND_LINE=\"${bwa_commandline}\" \\\n",
      "        PROGRAM_GROUP_NAME=\"bwamem\" \\\n",
      "        UNMAPPED_READ_STRATEGY=COPY_TO_TAG \\\n",
      "        ALIGNER_PROPER_PAIR_FLAGS=true \\\n",
      "        UNMAP_CONTAMINANT_READS=true \\\n",
      "        ADD_PG_TAG_TO_READS=false\n",
      "\n",
      "      grep -m1 \"read .* ALT contigs\" ${output_bam_basename}.bwa.stderr.log | \\\n",
      "      grep -v \"read 0 ALT contigs\"\n",
      "\n",
      "    # else ref_alt is empty or could not be found\n",
      "    else\n",
      "      exit 1;\n",
      "    fi\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"14 GB\"\n",
      "    cpu: \"16\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_bam = \"${output_bam_basename}.bam\"\n",
      "    File bwa_stderr_log = \"${output_bam_basename}.bwa.stderr.log\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Sort BAM file by coordinate order and fix tag values for NM and UQ\n",
      "task SortSam {\n",
      "  File input_bam\n",
      "  String output_bam_basename\n",
      "  Int preemptible_tries\n",
      "  Int compression_level\n",
      "  Float disk_size\n",
      "\n",
      "  command {\n",
      "    java -Dsamjdk.compression_level=${compression_level} -Xms4000m -jar /usr/gitc/picard.jar \\\n",
      "      SortSam \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OUTPUT=${output_bam_basename}.bam \\\n",
      "      SORT_ORDER=\"coordinate\" \\\n",
      "      CREATE_INDEX=true \\\n",
      "      CREATE_MD5_FILE=true \\\n",
      "      MAX_RECORDS_IN_RAM=300000\n",
      "  }\n",
      "  runtime {\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    cpu: \"1\"\n",
      "    memory: \"5000 MB\"\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_bam = \"${output_bam_basename}.bam\"\n",
      "    File output_bam_index = \"${output_bam_basename}.bai\"\n",
      "    File output_bam_md5 = \"${output_bam_basename}.bam.md5\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Collect base quality and insert size metrics\n",
      "task CollectUnsortedReadgroupBamQualityMetrics {\n",
      "  File input_bam\n",
      "  String output_bam_prefix\n",
      "  Int preemptible_tries\n",
      "  Float disk_size\n",
      "\n",
      "  command {\n",
      "    java -Xms5000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectMultipleMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OUTPUT=${output_bam_prefix} \\\n",
      "      ASSUME_SORTED=true \\\n",
      "      PROGRAM=\"null\" \\\n",
      "      PROGRAM=\"CollectBaseDistributionByCycle\" \\\n",
      "      PROGRAM=\"CollectInsertSizeMetrics\" \\\n",
      "      PROGRAM=\"MeanQualityByCycle\" \\\n",
      "      PROGRAM=\"QualityScoreDistribution\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"null\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"ALL_READS\"\n",
      "\n",
      "    touch ${output_bam_prefix}.insert_size_metrics\n",
      "    touch ${output_bam_prefix}.insert_size_histogram.pdf\n",
      "  }\n",
      "  runtime {\n",
      "    memory: \"7 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File base_distribution_by_cycle_pdf = \"${output_bam_prefix}.base_distribution_by_cycle.pdf\"\n",
      "    File base_distribution_by_cycle_metrics = \"${output_bam_prefix}.base_distribution_by_cycle_metrics\"\n",
      "    File insert_size_histogram_pdf = \"${output_bam_prefix}.insert_size_histogram.pdf\"\n",
      "    File insert_size_metrics = \"${output_bam_prefix}.insert_size_metrics\"\n",
      "    File quality_by_cycle_pdf = \"${output_bam_prefix}.quality_by_cycle.pdf\"\n",
      "    File quality_by_cycle_metrics = \"${output_bam_prefix}.quality_by_cycle_metrics\"\n",
      "    File quality_distribution_pdf = \"${output_bam_prefix}.quality_distribution.pdf\"\n",
      "    File quality_distribution_metrics = \"${output_bam_prefix}.quality_distribution_metrics\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Collect alignment summary and GC bias quality metrics\n",
      "task CollectReadgroupBamQualityMetrics {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String output_bam_prefix\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Int preemptible_tries\n",
      "  Float disk_size\n",
      "\n",
      "  command {\n",
      "    java -Xms5000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectMultipleMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "      OUTPUT=${output_bam_prefix} \\\n",
      "      ASSUME_SORTED=true \\\n",
      "      PROGRAM=\"null\" \\\n",
      "      PROGRAM=\"CollectAlignmentSummaryMetrics\" \\\n",
      "      PROGRAM=\"CollectGcBiasMetrics\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"null\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"READ_GROUP\"\n",
      "  }\n",
      "  runtime {\n",
      "    memory: \"7 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File alignment_summary_metrics = \"${output_bam_prefix}.alignment_summary_metrics\"\n",
      "    File gc_bias_detail_metrics = \"${output_bam_prefix}.gc_bias.detail_metrics\"\n",
      "    File gc_bias_pdf = \"${output_bam_prefix}.gc_bias.pdf\"\n",
      "    File gc_bias_summary_metrics = \"${output_bam_prefix}.gc_bias.summary_metrics\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Collect quality metrics from the aggregated bam\n",
      "task CollectAggregationMetrics {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String output_bam_prefix\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Int preemptible_tries\n",
      "  Float disk_size\n",
      "\n",
      "  command {\n",
      "    java -Xms5000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectMultipleMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "      OUTPUT=${output_bam_prefix} \\\n",
      "      ASSUME_SORTED=true \\\n",
      "      PROGRAM=\"null\" \\\n",
      "      PROGRAM=\"CollectAlignmentSummaryMetrics\" \\\n",
      "      PROGRAM=\"CollectInsertSizeMetrics\" \\\n",
      "      PROGRAM=\"CollectSequencingArtifactMetrics\" \\\n",
      "      PROGRAM=\"CollectGcBiasMetrics\" \\\n",
      "      PROGRAM=\"QualityScoreDistribution\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"null\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"SAMPLE\" \\\n",
      "      METRIC_ACCUMULATION_LEVEL=\"LIBRARY\"\n",
      "\n",
      "    touch ${output_bam_prefix}.insert_size_metrics\n",
      "    touch ${output_bam_prefix}.insert_size_histogram.pdf\n",
      "  }\n",
      "  runtime {\n",
      "    memory: \"7 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File alignment_summary_metrics = \"${output_bam_prefix}.alignment_summary_metrics\"\n",
      "    File bait_bias_detail_metrics = \"${output_bam_prefix}.bait_bias_detail_metrics\"\n",
      "    File bait_bias_summary_metrics = \"${output_bam_prefix}.bait_bias_summary_metrics\"\n",
      "    File gc_bias_detail_metrics = \"${output_bam_prefix}.gc_bias.detail_metrics\"\n",
      "    File gc_bias_pdf = \"${output_bam_prefix}.gc_bias.pdf\"\n",
      "    File gc_bias_summary_metrics = \"${output_bam_prefix}.gc_bias.summary_metrics\"\n",
      "    File insert_size_histogram_pdf = \"${output_bam_prefix}.insert_size_histogram.pdf\"\n",
      "    File insert_size_metrics = \"${output_bam_prefix}.insert_size_metrics\"\n",
      "    File pre_adapter_detail_metrics = \"${output_bam_prefix}.pre_adapter_detail_metrics\"\n",
      "    File pre_adapter_summary_metrics = \"${output_bam_prefix}.pre_adapter_summary_metrics\"\n",
      "    File quality_distribution_pdf = \"${output_bam_prefix}.quality_distribution.pdf\"\n",
      "    File quality_distribution_metrics = \"${output_bam_prefix}.quality_distribution_metrics\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Check that the fingerprints of separate readgroups all match\n",
      "task CrossCheckFingerprints {\n",
      "  Array[File] input_bams\n",
      "  Array[File] input_bam_indexes\n",
      "  File? haplotype_database_file\n",
      "  String metrics_filename\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "    java -Dsamjdk.buffer_size=131072 \\\n",
      "      -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms2000m \\\n",
      "      -jar /usr/gitc/picard.jar \\\n",
      "      CrosscheckReadGroupFingerprints \\\n",
      "      OUTPUT=${metrics_filename} \\\n",
      "      HAPLOTYPE_MAP=${haplotype_database_file} \\\n",
      "      EXPECT_ALL_READ_GROUPS_TO_MATCH=true \\\n",
      "      INPUT=${sep=' INPUT=' input_bams} \\\n",
      "      LOD_THRESHOLD=-20.0\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"2 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File metrics = \"${metrics_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Check that the fingerprint of the sample BAM matches the sample array\n",
      "task CheckFingerprint {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String output_basename\n",
      "  File? haplotype_database_file\n",
      "  File? genotypes\n",
      "  String sample\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "    java -Dsamjdk.buffer_size=131072 \\\n",
      "      -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms1024m  \\\n",
      "      -jar /usr/gitc/picard.jar \\\n",
      "      CheckFingerprint \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OUTPUT=${output_basename} \\\n",
      "      GENOTYPES=${genotypes} \\\n",
      "      HAPLOTYPE_MAP=${haplotype_database_file} \\\n",
      "      SAMPLE_ALIAS=\"${sample}\" \\\n",
      "      IGNORE_READ_GROUPS=true\n",
      "\n",
      "  >>>\n",
      " runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"1 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File summary_metrics = \"${output_basename}.fingerprinting_summary_metrics\"\n",
      "    File detail_metrics = \"${output_basename}.fingerprinting_detail_metrics\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Mark duplicate reads to avoid counting non-independent observations\n",
      "task MarkDuplicates {\n",
      "  Array[File] input_bams\n",
      "  String output_bam_basename\n",
      "  String metrics_filename\n",
      "  Float disk_size\n",
      "  Int compression_level\n",
      "  Int preemptible_tries\n",
      "\n",
      "  # The program default for READ_NAME_REGEX is appropriate in nearly every case.\n",
      "  # Sometimes we wish to supply \"null\" in order to turn off optical duplicate detection\n",
      "  # This can be desirable if you don't mind the estimated library size being wrong and optical duplicate detection is taking >7 days and failing\n",
      "  String? read_name_regex\n",
      "\n",
      " # Task is assuming query-sorted input so that the Secondary and Supplementary reads get marked correctly\n",
      " # This works because the output of BWA is query-grouped and therefore, so is the output of MergeBamAlignment.\n",
      " # While query-grouped isn't actually query-sorted, it's good enough for MarkDuplicates with ASSUME_SORT_ORDER=\"queryname\"\n",
      "  command {\n",
      "    java -Dsamjdk.compression_level=${compression_level} -Xms4000m -jar /usr/gitc/picard.jar \\\n",
      "      MarkDuplicates \\\n",
      "      INPUT=${sep=' INPUT=' input_bams} \\\n",
      "      OUTPUT=${output_bam_basename}.bam \\\n",
      "      METRICS_FILE=${metrics_filename} \\\n",
      "      VALIDATION_STRINGENCY=SILENT \\\n",
      "      ${\"READ_NAME_REGEX=\" + read_name_regex} \\\n",
      "      OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500 \\\n",
      "      ASSUME_SORT_ORDER=\"queryname\" \\\n",
      "      CLEAR_DT=\"false\" \\\n",
      "      ADD_PG_TAG_TO_READS=false\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"7 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_bam = \"${output_bam_basename}.bam\"\n",
      "    File duplicate_metrics = \"${metrics_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Generate sets of intervals for scatter-gathering over chromosomes\n",
      "task CreateSequenceGroupingTSV {\n",
      "  File ref_dict\n",
      "  Int preemptible_tries\n",
      "\n",
      "  # Use python to create the Sequencing Groupings used for BQSR and PrintReads Scatter.\n",
      "  # It outputs to stdout where it is parsed into a wdl Array[Array[String]]\n",
      "  # e.g. [[\"1\"], [\"2\"], [\"3\", \"4\"], [\"5\"], [\"6\", \"7\", \"8\"]]\n",
      "  command <<<\n",
      "    python <<CODE\n",
      "    with open(\"${ref_dict}\", \"r\") as ref_dict_file:\n",
      "        sequence_tuple_list = []\n",
      "        longest_sequence = 0\n",
      "        for line in ref_dict_file:\n",
      "            if line.startswith(\"@SQ\"):\n",
      "                line_split = line.split(\"\\t\")\n",
      "                # (Sequence_Name, Sequence_Length)\n",
      "                sequence_tuple_list.append((line_split[1].split(\"SN:\")[1], int(line_split[2].split(\"LN:\")[1])))\n",
      "        longest_sequence = sorted(sequence_tuple_list, key=lambda x: x[1], reverse=True)[0][1]\n",
      "    # We are adding this to the intervals because hg38 has contigs named with embedded colons and a bug in GATK strips off\n",
      "    # the last element after a :, so we add this as a sacrificial element.\n",
      "    hg38_protection_tag = \":1+\"\n",
      "    # initialize the tsv string with the first sequence\n",
      "    tsv_string = sequence_tuple_list[0][0] + hg38_protection_tag\n",
      "    temp_size = sequence_tuple_list[0][1]\n",
      "    for sequence_tuple in sequence_tuple_list[1:]:\n",
      "        if temp_size + sequence_tuple[1] <= longest_sequence:\n",
      "            temp_size += sequence_tuple[1]\n",
      "            tsv_string += \"\\t\" + sequence_tuple[0] + hg38_protection_tag\n",
      "        else:\n",
      "            tsv_string += \"\\n\" + sequence_tuple[0] + hg38_protection_tag\n",
      "            temp_size = sequence_tuple[1]\n",
      "    # add the unmapped sequences as a separate line to ensure that they are recalibrated as well\n",
      "    with open(\"sequence_grouping.txt\",\"w\") as tsv_file:\n",
      "      tsv_file.write(tsv_string)\n",
      "      tsv_file.close()\n",
      "\n",
      "    tsv_string += '\\n' + \"unmapped\"\n",
      "\n",
      "    with open(\"sequence_grouping_with_unmapped.txt\",\"w\") as tsv_file_with_unmapped:\n",
      "      tsv_file_with_unmapped.write(tsv_string)\n",
      "      tsv_file_with_unmapped.close()\n",
      "    CODE\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"python:2.7\"\n",
      "    memory: \"2 GB\"\n",
      "  }\n",
      "  output {\n",
      "    Array[Array[String]] sequence_grouping = read_tsv(\"sequence_grouping.txt\")\n",
      "    Array[Array[String]] sequence_grouping_with_unmapped = read_tsv(\"sequence_grouping_with_unmapped.txt\")\n",
      "  }\n",
      "}\n",
      "\n",
      "# Generate Base Quality Score Recalibration (BQSR) model\n",
      "task BaseRecalibrator {\n",
      "  String input_bam\n",
      "  String recalibration_report_filename\n",
      "  Array[String] sequence_group_interval\n",
      "  File dbSNP_vcf\n",
      "  File dbSNP_vcf_index\n",
      "  Array[File] known_indels_sites_VCFs\n",
      "  Array[File] known_indels_sites_indices\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    /gatk/gatk --java-options \"-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \\\n",
      "      -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \\\n",
      "      -Xloggc:gc_log.log -Xms4000m\" \\\n",
      "      BaseRecalibrator \\\n",
      "      -R ${ref_fasta} \\\n",
      "      -I ${input_bam} \\\n",
      "      --use-original-qualities \\\n",
      "      -O ${recalibration_report_filename} \\\n",
      "      --known-sites ${dbSNP_vcf} \\\n",
      "      --known-sites ${sep=\" --known-sites \" known_indels_sites_VCFs} \\\n",
      "      -L ${sep=\" -L \" sequence_group_interval}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"6 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "  }\n",
      "  output {\n",
      "    File recalibration_report = \"${recalibration_report_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Apply Base Quality Score Recalibration (BQSR) model\n",
      "task ApplyBQSR {\n",
      "  String input_bam\n",
      "  String output_bam_basename\n",
      "  File recalibration_report\n",
      "  Array[String] sequence_group_interval\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Float disk_size\n",
      "  Int compression_level\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    /gatk/gatk --java-options \"-XX:+PrintFlagsFinal -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps \\\n",
      "      -XX:+PrintGCDetails -Xloggc:gc_log.log \\\n",
      "      -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Dsamjdk.compression_level=${compression_level} -Xms3000m\" \\\n",
      "      ApplyBQSR \\\n",
      "      --create-output-bam-md5 \\\n",
      "      --add-output-sam-program-record \\\n",
      "      -R ${ref_fasta} \\\n",
      "      -I ${input_bam} \\\n",
      "      --use-original-qualities \\\n",
      "      -O ${output_bam_basename}.bam \\\n",
      "      -bqsr ${recalibration_report} \\\n",
      "      --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 \\\n",
      "      -L ${sep=\" -L \" sequence_group_interval}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3500 MB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "  }\n",
      "  output {\n",
      "    File recalibrated_bam = \"${output_bam_basename}.bam\"\n",
      "    File recalibrated_bam_checksum = \"${output_bam_basename}.bam.md5\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Combine multiple recalibration tables from scattered BaseRecalibrator runs\n",
      "task GatherBqsrReports {\n",
      "  Array[File] input_bqsr_reports\n",
      "  String output_report_filename\n",
      "  Int disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    /gatk/gatk --java-options \"-Xms3000m\" \\\n",
      "      GatherBQSRReports \\\n",
      "      -I ${sep=' -I ' input_bqsr_reports} \\\n",
      "      -O ${output_report_filename}\n",
      "    }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3500 MB\"\n",
      "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
      "  }\n",
      "  output {\n",
      "    File output_bqsr_report = \"${output_report_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Combine multiple recalibrated BAM files from scattered ApplyRecalibration runs\n",
      "task GatherBamFiles {\n",
      "  Array[File] input_bams\n",
      "  String output_bam_basename\n",
      "  Float disk_size\n",
      "  Int compression_level\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Dsamjdk.compression_level=${compression_level} -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      GatherBamFiles \\\n",
      "      INPUT=${sep=' INPUT=' input_bams} \\\n",
      "      OUTPUT=${output_bam_basename}.bam \\\n",
      "      CREATE_INDEX=true \\\n",
      "      CREATE_MD5_FILE=true\n",
      "    }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_bam = \"${output_bam_basename}.bam\"\n",
      "    File output_bam_index = \"${output_bam_basename}.bai\"\n",
      "    File output_bam_md5 = \"${output_bam_basename}.bam.md5\"\n",
      "  }\n",
      "}\n",
      "\n",
      "task CheckPreValidation {\n",
      "    File duplication_metrics\n",
      "    File chimerism_metrics\n",
      "    Float max_duplication_in_reasonable_sample\n",
      "    Float max_chimerism_in_reasonable_sample\n",
      "    Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "    set -o pipefail\n",
      "    set -e\n",
      "\n",
      "    grep -A 1 PERCENT_DUPLICATION ${duplication_metrics} > duplication.csv\n",
      "    grep -A 3 PCT_CHIMERAS ${chimerism_metrics} | grep -v OF_PAIR > chimerism.csv\n",
      "\n",
      "    python <<CODE\n",
      "\n",
      "    import csv\n",
      "    with open('duplication.csv') as dupfile:\n",
      "      reader = csv.DictReader(dupfile, delimiter='\\t')\n",
      "      for row in reader:\n",
      "        with open(\"duplication_value.txt\",\"w\") as file:\n",
      "          file.write(row['PERCENT_DUPLICATION'])\n",
      "          file.close()\n",
      "\n",
      "    with open('chimerism.csv') as chimfile:\n",
      "      reader = csv.DictReader(chimfile, delimiter='\\t')\n",
      "      for row in reader:\n",
      "        with open(\"chimerism_value.txt\",\"w\") as file:\n",
      "          file.write(row['PCT_CHIMERAS'])\n",
      "          file.close()\n",
      "\n",
      "    CODE\n",
      "\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    docker: \"python:2.7\"\n",
      "    memory: \"2 GB\"\n",
      "  }\n",
      "  output {\n",
      "    Float duplication_rate = read_float(\"duplication_value.txt\")\n",
      "    Float chimerism_rate = read_float(\"chimerism_value.txt\")\n",
      "    Boolean is_outlier_data = duplication_rate > max_duplication_in_reasonable_sample || chimerism_rate > max_chimerism_in_reasonable_sample\n",
      "  }\n",
      "}\n",
      "\n",
      "task ValidateSamFile {\n",
      "  File input_bam\n",
      "  File? input_bam_index\n",
      "  String report_filename\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Int? max_output\n",
      "  Array[String]? ignore\n",
      "  Boolean? is_outlier_data\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms6000m -jar /usr/gitc/picard.jar \\\n",
      "      ValidateSamFile \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OUTPUT=${report_filename} \\\n",
      "      REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "      ${\"MAX_OUTPUT=\" + max_output} \\\n",
      "      IGNORE=${default=\"null\" sep=\" IGNORE=\" ignore} \\\n",
      "      MODE=VERBOSE \\\n",
      "      ${default='SKIP_MATE_VALIDATION=false' true='SKIP_MATE_VALIDATION=true' false='SKIP_MATE_VALIDATION=false' is_outlier_data} \\\n",
      "      IS_BISULFITE_SEQUENCED=false\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"7 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File report = \"${report_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Note these tasks will break if the read lengths in the bam are greater than 250.\n",
      "task CollectWgsMetrics {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String metrics_filename\n",
      "  File wgs_coverage_interval_list\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Int? read_length\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectWgsMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      VALIDATION_STRINGENCY=SILENT \\\n",
      "      REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "      INCLUDE_BQ_HISTOGRAM=true \\\n",
      "      INTERVALS=${wgs_coverage_interval_list} \\\n",
      "      OUTPUT=${metrics_filename} \\\n",
      "      USE_FAST_ALGORITHM=true \\\n",
      "      READ_LENGTH=${default=250 read_length}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File metrics = \"${metrics_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Collect raw WGS metrics (commonly used QC thresholds)\n",
      "task CollectRawWgsMetrics {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String metrics_filename\n",
      "  File wgs_coverage_interval_list\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Int? read_length\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectRawWgsMetrics \\\n",
      "      INPUT=${input_bam} \\\n",
      "      VALIDATION_STRINGENCY=SILENT \\\n",
      "      REFERENCE_SEQUENCE=${ref_fasta} \\\n",
      "      INCLUDE_BQ_HISTOGRAM=true \\\n",
      "      INTERVALS=${wgs_coverage_interval_list} \\\n",
      "      OUTPUT=${metrics_filename} \\\n",
      "      USE_FAST_ALGORITHM=true \\\n",
      "      READ_LENGTH=${default=250 read_length}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File metrics = \"${metrics_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Generate a checksum per readgroup\n",
      "task CalculateReadGroupChecksum {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  String read_group_md5_filename\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms1000m -jar /usr/gitc/picard.jar \\\n",
      "      CalculateReadGroupChecksum \\\n",
      "      INPUT=${input_bam} \\\n",
      "      OUTPUT=${read_group_md5_filename}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"2 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File md5_file = \"${read_group_md5_filename}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Notes on the contamination estimate:\n",
      "# The contamination value is read from the FREEMIX field of the selfSM file output by verifyBamId\n",
      "#\n",
      "# In Zamboni production, this value is stored directly in METRICS.AGGREGATION_CONTAM\n",
      "#\n",
      "# Contamination is also stored in GVCF_CALLING and thereby passed to HAPLOTYPE_CALLER\n",
      "# But first, it is divided by an underestimation factor thusly:\n",
      "#   float(FREEMIX) / ContaminationUnderestimationFactor\n",
      "#     where the denominator is hardcoded in Zamboni:\n",
      "#     val ContaminationUnderestimationFactor = 0.75f\n",
      "#\n",
      "# Here, I am handling this by returning both the original selfSM file for reporting, and the adjusted\n",
      "# contamination estimate for use in variant calling\n",
      "task CheckContamination {\n",
      "  File input_bam\n",
      "  File input_bam_index\n",
      "  File contamination_sites_ud\n",
      "  File contamination_sites_bed\n",
      "  File contamination_sites_mu\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  String output_prefix\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "  Float contamination_underestimation_factor\n",
      "\n",
      "  command <<<\n",
      "    set -e\n",
      "\n",
      "    # creates a ${output_prefix}.selfSM file, a TSV file with 2 rows, 19 columns.\n",
      "    # First row are the keys (e.g., SEQ_SM, RG, FREEMIX), second row are the associated values\n",
      "    /usr/gitc/VerifyBamID \\\n",
      "    --Verbose \\\n",
      "    --NumPC 4 \\\n",
      "    --Output ${output_prefix} \\\n",
      "    --BamFile ${input_bam} \\\n",
      "    --Reference ${ref_fasta} \\\n",
      "    --UDPath ${contamination_sites_ud} \\\n",
      "    --MeanPath ${contamination_sites_mu} \\\n",
      "    --BedPath ${contamination_sites_bed} \\\n",
      "    1>/dev/null\n",
      "\n",
      "    # used to read from the selfSM file and calculate contamination, which gets printed out\n",
      "    python3 <<CODE\n",
      "    import csv\n",
      "    import sys\n",
      "    with open('${output_prefix}.selfSM') as selfSM:\n",
      "      reader = csv.DictReader(selfSM, delimiter='\\t')\n",
      "      i = 0\n",
      "      for row in reader:\n",
      "        if float(row[\"FREELK0\"])==0 and float(row[\"FREELK1\"])==0:\n",
      "          # a zero value for the likelihoods implies no data. This usually indicates a problem rather than a real event.\n",
      "          # if the bam isn't really empty, this is probably due to the use of a incompatible reference build between\n",
      "          # vcf and bam.\n",
      "          sys.stderr.write(\"Found zero likelihoods. Bam is either very-very shallow, or aligned to the wrong reference (relative to the vcf).\")\n",
      "          sys.exit(1)\n",
      "        print(float(row[\"FREEMIX\"])/${contamination_underestimation_factor})\n",
      "        i = i + 1\n",
      "        # there should be exactly one row, and if this isn't the case the format of the output is unexpectedly different\n",
      "        # and the results are not reliable.\n",
      "        if i != 1:\n",
      "          sys.stderr.write(\"Found %d rows in .selfSM file. Was expecting exactly 1. This is an error\"%(i))\n",
      "          sys.exit(2)\n",
      "    CODE\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"2 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"us.gcr.io/broad-gotc-prod/verify-bam-id:c8a66425c312e5f8be46ab0c41f8d7a1942b6e16-1500298351\"\n",
      "  }\n",
      "  output {\n",
      "    File selfSM = \"${output_prefix}.selfSM\"\n",
      "    Float contamination = read_float(stdout())\n",
      "  }\n",
      "}\n",
      "\n",
      "# This task calls picard's IntervalListTools to scatter the input interval list into scatter_count sub interval lists\n",
      "# Note that the number of sub interval lists may not be exactly equal to scatter_count.  There may be slightly more or less.\n",
      "# Thus we have the block of python to count the number of generated sub interval lists.\n",
      "task ScatterIntervalList {\n",
      "  File interval_list\n",
      "  Int scatter_count\n",
      "  Int break_bands_at_multiples_of\n",
      "\n",
      "  command <<<\n",
      "    set -e\n",
      "    mkdir out\n",
      "    java -Xms1g -jar /usr/gitc/picard.jar \\\n",
      "      IntervalListTools \\\n",
      "      SCATTER_COUNT=${scatter_count} \\\n",
      "      SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \\\n",
      "      UNIQUE=true \\\n",
      "      SORT=true \\\n",
      "      BREAK_BANDS_AT_MULTIPLES_OF=${break_bands_at_multiples_of} \\\n",
      "      INPUT=${interval_list} \\\n",
      "      OUTPUT=out\n",
      "\n",
      "    python3 <<CODE\n",
      "    import glob, os\n",
      "    # Works around a JES limitation where multiples files with the same name overwrite each other when globbed\n",
      "    intervals = sorted(glob.glob(\"out/*/*.interval_list\"))\n",
      "    for i, interval in enumerate(intervals):\n",
      "      (directory, filename) = os.path.split(interval)\n",
      "      newName = os.path.join(directory, str(i + 1) + filename)\n",
      "      os.rename(interval, newName)\n",
      "    print(len(intervals))\n",
      "    CODE\n",
      "  >>>\n",
      "  output {\n",
      "    Array[File] out = glob(\"out/*/*.interval_list\")\n",
      "    Int interval_count = read_int(stdout())\n",
      "  }\n",
      "  runtime {\n",
      "    memory: \"2 GB\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Call variants on a single sample with HaplotypeCaller to produce a GVCF\n",
      "task HaplotypeCaller {\n",
      "  String input_bam\n",
      "  File interval_list\n",
      "  String gvcf_basename\n",
      "  File ref_dict\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  Float? contamination\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  # We use interval_padding 500 below to make sure that the HaplotypeCaller has context on both sides around\n",
      "  # the interval because the assembly uses them.\n",
      "  #\n",
      "  # Using PrintReads is a temporary solution until we update HaploypeCaller to use GATK4. Once that is done,\n",
      "  # HaplotypeCaller can stream the required intervals directly from the cloud.\n",
      "  command {\n",
      "    /usr/gitc/gatk4/gatk-launch --javaOptions \"-Xms2g\" \\\n",
      "      PrintReads \\\n",
      "      -I ${input_bam} \\\n",
      "      --interval_padding 500 \\\n",
      "      -L ${interval_list} \\\n",
      "      -O local.sharded.bam \\\n",
      "    && \\\n",
      "    java -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xms8000m \\\n",
      "      -jar /usr/gitc/GATK35.jar \\\n",
      "      -T HaplotypeCaller \\\n",
      "      -R ${ref_fasta} \\\n",
      "      -o ${gvcf_basename}.vcf.gz \\\n",
      "      -I local.sharded.bam \\\n",
      "      -L ${interval_list} \\\n",
      "      -ERC GVCF \\\n",
      "      --max_alternate_alleles 3 \\\n",
      "      -variant_index_parameter 128000 \\\n",
      "      -variant_index_type LINEAR \\\n",
      "      -contamination ${default=0 contamination} \\\n",
      "      --read_filter OverclippedRead\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"10 GB\"\n",
      "    cpu: \"1\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_gvcf = \"${gvcf_basename}.vcf.gz\"\n",
      "    File output_gvcf_index = \"${gvcf_basename}.vcf.gz.tbi\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Combine multiple VCFs or GVCFs from scattered HaplotypeCaller runs\n",
      "task MergeVCFs {\n",
      "  Array[File] input_vcfs\n",
      "  Array[File] input_vcfs_indexes\n",
      "  String output_vcf_name\n",
      "  Int disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  # Using MergeVcfs instead of GatherVcfs so we can create indices\n",
      "  # See https://github.com/broadinstitute/picard/issues/789 for relevant GatherVcfs ticket\n",
      "  command {\n",
      "    java -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      MergeVcfs \\\n",
      "      INPUT=${sep=' INPUT=' input_vcfs} \\\n",
      "      OUTPUT=${output_vcf_name}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_vcf = \"${output_vcf_name}\"\n",
      "    File output_vcf_index = \"${output_vcf_name}.tbi\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Validate a GVCF with -gvcf specific validation\n",
      "task ValidateGVCF {\n",
      "  File input_vcf\n",
      "  File input_vcf_index\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  File ref_dict\n",
      "  File dbSNP_vcf\n",
      "  File dbSNP_vcf_index\n",
      "  File wgs_calling_interval_list\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    /gatk/gatk --java-options \"-Xms3000m\" \\\n",
      "      ValidateVariants \\\n",
      "      -V ${input_vcf} \\\n",
      "      -R ${ref_fasta} \\\n",
      "      -L ${wgs_calling_interval_list} \\\n",
      "      -gvcf \\\n",
      "      --validation-type-to-exclude ALLELES \\\n",
      "      --dbsnp ${dbSNP_vcf}\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3500 MB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Collect variant calling metrics from GVCF output\n",
      "task CollectGvcfCallingMetrics {\n",
      "  File input_vcf\n",
      "  File input_vcf_index\n",
      "  String metrics_basename\n",
      "  File dbSNP_vcf\n",
      "  File dbSNP_vcf_index\n",
      "  File ref_dict\n",
      "  File wgs_evaluation_interval_list\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command {\n",
      "    java -Xms2000m -jar /usr/gitc/picard.jar \\\n",
      "      CollectVariantCallingMetrics \\\n",
      "      INPUT=${input_vcf} \\\n",
      "      OUTPUT=${metrics_basename} \\\n",
      "      DBSNP=${dbSNP_vcf} \\\n",
      "      SEQUENCE_DICTIONARY=${ref_dict} \\\n",
      "      TARGET_INTERVALS=${wgs_evaluation_interval_list} \\\n",
      "      GVCF_INPUT=true\n",
      "  }\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File summary_metrics = \"${metrics_basename}.variant_calling_summary_metrics\"\n",
      "    File detail_metrics = \"${metrics_basename}.variant_calling_detail_metrics\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Convert BAM file to CRAM format\n",
      "# Note that reading CRAMs directly with Picard is not yet supported\n",
      "task ConvertToCram {\n",
      "  File input_bam\n",
      "  File ref_fasta\n",
      "  File ref_fasta_index\n",
      "  String output_basename\n",
      "  Float disk_size\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "    set -e\n",
      "    set -o pipefail\n",
      "\n",
      "    samtools view -C -T ${ref_fasta} ${input_bam} | \\\n",
      "    tee ${output_basename}.cram | \\\n",
      "    md5sum | awk '{print $1}' > ${output_basename}.cram.md5\n",
      "\n",
      "    # Create REF_CACHE. Used when indexing a CRAM\n",
      "    seq_cache_populate.pl -root ./ref/cache ${ref_fasta}\n",
      "    export REF_PATH=:\n",
      "    export REF_CACHE=./ref/cache/%2s/%2s/%s\n",
      "\n",
      "    samtools index ${output_basename}.cram\n",
      "  >>>\n",
      "  runtime {\n",
      "    preemptible: preemptible_tries\n",
      "    memory: \"3 GB\"\n",
      "    cpu: \"1\"\n",
      "    disks: \"local-disk \" + sub(disk_size, \"\\\\..*\", \"\") + \" HDD\"\n",
      "    docker: \"broadinstitute/genomes-in-the-cloud:2.3.1-1512499786\"\n",
      "  }\n",
      "  output {\n",
      "    File output_cram = \"${output_basename}.cram\"\n",
      "    File output_cram_index = \"${output_basename}.cram.crai\"\n",
      "    File output_cram_md5 = \"${output_basename}.cram.md5\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Calculates sum of a list of floats\n",
      "task SumFloats {\n",
      "  Array[Float] sizes\n",
      "  Int preemptible_tries\n",
      "\n",
      "  command <<<\n",
      "  python -c \"print ${sep=\"+\" sizes}\"\n",
      "  >>>\n",
      "  output {\n",
      "    Float total_size = read_float(stdout())\n",
      "  }\n",
      "  runtime {\n",
      "    docker: \"python:2.7\"\n",
      "    preemptible: preemptible_tries\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat broad-prod-wgs-germline-snps-indels/PairedEndSingleSampleWf.gatk4.0.wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Our options file looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"read_from_cache\":false,\n",
      "  \"default_runtime_attributes\": {\n",
      "    \"zones\": \"us-central1-a us-central1-b us-east1-d us-central1-c us-central1-f us-east1-c\",\n",
      "    \"docker\": \"broadinstitute/gatk:4.0.0.0\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat broad-prod-wgs-germline-snps-indels/PairedEndSingleSampleWf.gatk4.0.options.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "PprqqRKiVw_T"
   },
   "source": [
    "#### Create resources for outputs/logs etc. (if you don't have it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 54,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 3124,
     "status": "ok",
     "timestamp": 1519138787195,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "MOizqwrFVw_U",
    "outputId": "3162fb9b-abd9-420e-9944-efdee3cb42cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://genomics-labs/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCS=gs://genomics-labs/gatk4\n"
     ]
    }
   ],
   "source": [
    "%env GCS=gs://genomics-labs/gatk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GATK4_DIR=/content/datalab/notebooks/Genomics/GATK/broad-prod-wgs-germline-snps-indels\n"
     ]
    }
   ],
   "source": [
    "%env GATK4_DIR=/content/datalab/notebooks/Genomics/GATK/broad-prod-wgs-germline-snps-indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 35,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1519140502786,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "M7ie7psfVw_W",
    "outputId": "116639b7-f573-42e2-b929-2c3fc1bd0e8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/content/datalab/notebooks/Genomics/GATK'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "tvyhA5x-Vw_Y"
   },
   "source": [
    "#### configure the pipeline and kick it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://8080-dot-3009239-dot-devshell.appspot.com/",
     "height": 166,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 2262,
     "status": "ok",
     "timestamp": 1519141102353,
     "user": {
      "displayName": "Keith Binder",
      "userId": "113141324214637130304"
     },
     "user_tz": 300
    },
    "id": "XkToV-PfVw_Z",
    "outputId": "681b4963-3164-49ec-f7ea-c7bad7f86ad4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/EK2d-qebLBirs8i0qaDnwEsglOj89t8GKg9wcm9kdWN0aW9uUXVldWU].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud alpha genomics pipelines run \\\n",
    "  --pipeline-file /content/datalab/notebooks/Genomics/GATK/open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \\\n",
    "  --zones us-east1-c \\\n",
    "  --memory 5 \\\n",
    "  --logging $GCS/logging \\\n",
    "  --inputs-from-file WDL=$GATK4_DIR/PairedEndSingleSampleWf.gatk4.0.wdl \\\n",
    "  --inputs-from-file WORKFLOW_INPUTS=$GATK4_DIR/PairedEndSingleSampleWf.hg38.inputs.json \\\n",
    "  --inputs-from-file WORKFLOW_OPTIONS=$GATK4_DIR/PairedEndSingleSampleWf.gatk4.0.options.json \\\n",
    "  --inputs WORKSPACE=$GCS/workspace \\\n",
    "  --inputs OUTPUTS=$GCS/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "zirzGTOqVw_b"
   },
   "source": [
    "#### verify status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "lmkrRAN7Vw_c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "metadata:\n",
      "  events:\n",
      "  - description: start\n",
      "    startTime: '2018-02-20T20:38:44.374468118Z'\n",
      "  - description: pulling-image\n",
      "    startTime: '2018-02-20T20:38:44.374545767Z'\n",
      "  - description: localizing-files\n",
      "    startTime: '2018-02-20T20:39:19.251328689Z'\n",
      "  - description: running-docker\n",
      "    startTime: '2018-02-20T20:39:19.251380153Z'\n",
      "  - description: delocalizing-files\n",
      "    startTime: '2018-02-20T21:47:29.129516274Z'\n",
      "  - description: ok\n",
      "    startTime: '2018-02-20T21:47:30.451156310Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "gcloud alpha genomics operations describe ELanwaebLBi--dL-v9e_1MoBIJTo_PbfBioPcHJvZHVjdGlvblF1ZXVl --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "q_dAy8MjVw_f"
   },
   "source": [
    "#### Verify pipeline stages in console and in the bucket. Outputs consist of:\n",
    "- Cram, cram index, and cram md5 \n",
    "- GVCF and its gvcf index \n",
    "- BQSR Report\n",
    "- Several Summary Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "pLguP0vWVw_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://genomics-labs/gatk4/logging/\n",
      "gs://genomics-labs/gatk4/outputs/\n",
      "gs://genomics-labs/gatk4/temp/\n",
      "gs://genomics-labs/gatk4/workspace/\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "gsutil ls $GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "v3e4F_lAVw_i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.insert_size_histogram.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.insert_size_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_distribution.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_distribution_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.1.ATCACGAT.20k_reads.unmapped.quality_yield_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.insert_size_histogram.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.insert_size_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.quality_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.quality_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.quality_distribution.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.readgroup.quality_distribution_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06HDADXX130110.2.ATCACGAT.20k_reads.unmapped.quality_yield_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.base_distribution_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.insert_size_histogram.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.insert_size_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_by_cycle.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_by_cycle_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_distribution.pdf\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.readgroup.quality_distribution_metrics\n",
      "gs://genomics-labs/gatk4/outputs/H06JUADXX130110.1.ATCACGAT.20k_reads.unmapped.quality_yield_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.aligned.duplicates_marked.recalibrated.bam.read_group_md5\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.alignment_summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.bait_bias_detail_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.bait_bias_summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.cram\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.cram.crai\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.cram.md5\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.cram.validation_report\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.duplicate_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.g.vcf.gz\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.g.vcf.gz.tbi\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.gc_bias.detail_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.gc_bias.pdf\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.gc_bias.summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.insert_size_histogram.pdf\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.insert_size_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.preBqsr.selfSM\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.pre_adapter_detail_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.pre_adapter_summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.quality_distribution.pdf\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.quality_distribution_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.raw_wgs_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.readgroup.alignment_summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.readgroup.gc_bias.detail_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.readgroup.gc_bias.pdf\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.readgroup.gc_bias.summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.recal_data.csv\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.variant_calling_detail_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.variant_calling_summary_metrics\n",
      "gs://genomics-labs/gatk4/outputs/NA12878.wgs_metrics\n",
      "gs://genomics-labs/gatk4/outputs/wdl_run_metadata.json\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "gsutil ls $GCS/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gatk3-4-rnaseq-germline-snps-indels'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/gatk-workflows/gatk3-4-rnaseq-germline-snps-indels.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCS=gs://genomics-labs/gatk4/rnaseq\n"
     ]
    }
   ],
   "source": [
    "%env GCS=gs://genomics-labs/gatk4/rnaseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RNA_DIR=/content/datalab/notebooks/Genomics/GATK/gatk3-4-rnaseq-germline-snps-indels\n"
     ]
    }
   ],
   "source": [
    "%env RNA_DIR=/content/datalab/notebooks/Genomics/GATK/gatk3-4-rnaseq-germline-snps-indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/ELuz0sWvLBi02KeSwL2Ymy0glOj89t8GKg9wcm9kdWN0aW9uUXVldWU].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud alpha genomics pipelines run \\\n",
    "  --pipeline-file /content/datalab/notebooks/Genomics/GATK/open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \\\n",
    "  --zones us-east1-c \\\n",
    "  --memory 5 \\\n",
    "  --logging $GCS/logging \\\n",
    "  --inputs-from-file WDL=$RNA_DIR/rna-germline-variant-calling.wdl \\\n",
    "  --inputs-from-file WORKFLOW_INPUTS=$RNA_DIR/rna-germline-variant-calling.inputs.json \\\n",
    "  --inputs-from-file WORKFLOW_OPTIONS=$GATK4_DIR/PairedEndSingleSampleWf.gatk4.0.options.json \\\n",
    "  --inputs WORKSPACE=$GCS/workspace \\\n",
    "  --inputs OUTPUTS=$GCS/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "metadata:\n",
      "  events:\n",
      "  - description: start\n",
      "    startTime: '2018-04-24T17:30:27.281378912Z'\n",
      "  - description: pulling-image\n",
      "    startTime: '2018-04-24T17:30:27.281446606Z'\n",
      "  - description: localizing-files\n",
      "    startTime: '2018-04-24T17:30:57.567922192Z'\n",
      "  - description: running-docker\n",
      "    startTime: '2018-04-24T17:30:57.567963138Z'\n",
      "  - description: delocalizing-files\n",
      "    startTime: '2018-04-24T22:24:59.525814164Z'\n",
      "  - description: ok\n",
      "    startTime: '2018-04-24T22:25:00.946193988Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "gcloud alpha genomics operations describe ELuz0sWvLBi02KeSwL2Ymy0glOj89t8GKg9wcm9kdWN0aW9uUXVldWU --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gatk4-somatic-snvs-indels'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/gatk-workflows/gatk4-somatic-snvs-indels.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCS=gs://genomics-labs/gatk4/somatic\n"
     ]
    }
   ],
   "source": [
    "%env GCS=gs://genomics-labs/gatk4/somatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SOMATIC_DIR=/content/datalab/notebooks/Genomics/GATK/gatk4-somatic-snvs-indels\n"
     ]
    }
   ],
   "source": [
    "%env SOMATIC_DIR=/content/datalab/notebooks/Genomics/GATK/gatk4-somatic-snvs-indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/ELv3pPSvLBiEse_zsIWf9w0glOj89t8GKg9wcm9kdWN0aW9uUXVldWU].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud alpha genomics pipelines run \\\n",
    "  --pipeline-file /content/datalab/notebooks/Genomics/GATK/open_wdl/runners/cromwell_on_google/wdl_runner/wdl_pipeline.yaml \\\n",
    "  --zones us-east1-c \\\n",
    "  --memory 7 \\\n",
    "  --logging $GCS/logging \\\n",
    "  --inputs-from-file WDL=$SOMATIC_DIR/mutect2.wdl \\\n",
    "  --inputs-from-file WORKFLOW_INPUTS=$SOMATIC_DIR/mutect2.exome.inputs.json \\\n",
    "  --inputs-from-file WORKFLOW_OPTIONS=$GATK4_DIR/PairedEndSingleSampleWf.gatk4.0.options.json \\\n",
    "  --inputs WORKSPACE=$GCS/workspace \\\n",
    "  --inputs OUTPUTS=$GCS/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "eXiOLl5_Vw_l"
   },
   "source": [
    "![](https://tctechcrunch2011.files.wordpress.com/2017/01/verily.jpg?w=738)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## GenX secondary analysis on GCP - DeepVariant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "MYBjLQo2Vw_l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/notebooks/Genomics/GATK\n"
     ]
    }
   ],
   "source": [
    "cd /content/datalab/notebooks/Genomics/GATK/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "vBE8iGmyVw_s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "set -euo pipefail\n",
      "# Set common settings.\n",
      "PROJECT_ID=genomics-labs\n",
      "OUTPUT_BUCKET=gs://genomics-labs/DV\n",
      "STAGING_FOLDER_NAME=stage$random\n",
      "OUTPUT_FILE_NAME=output.vcf\n",
      "# Model for calling whole genome sequencing data.\n",
      "MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard\n",
      "# Model for calling exome sequencing data.\n",
      "# MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard\n",
      "IMAGE_VERSION=0.5.1\n",
      "DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:\"${IMAGE_VERSION}\"\n",
      "DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:\"${IMAGE_VERSION}\"\n",
      "\n",
      "# Run the pipeline.\n",
      "gcloud alpha genomics pipelines run \\\n",
      "  --project \"${PROJECT_ID}\" \\\n",
      "  --pipeline-file deepvariant_pipeline.yaml \\\n",
      "  --logging \"${OUTPUT_BUCKET}\"/runner_logs \\\n",
      "  --zones us-west1-b \\\n",
      "  --inputs `echo \\\n",
      "      PROJECT_ID=\"${PROJECT_ID}\", \\\n",
      "      OUTPUT_BUCKET=\"${OUTPUT_BUCKET}\", \\\n",
      "      MODEL=\"${MODEL}\", \\\n",
      "      DOCKER_IMAGE=\"${DOCKER_IMAGE}\", \\\n",
      "      DOCKER_IMAGE_GPU=\"${DOCKER_IMAGE_GPU}\", \\\n",
      "      STAGING_FOLDER_NAME=\"${STAGING_FOLDER_NAME}\", \\\n",
      "      OUTPUT_FILE_NAME=\"${OUTPUT_FILE_NAME}\" \\\n",
      "      | tr -d '[:space:]'`"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chmod u+x vdv.sh\n",
    "cat vdv.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "oAtcfvpqVw_w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: deepvariant_pipeline\n",
      "inputParameters:\n",
      "- name: PROJECT_ID\n",
      "- name: OUTPUT_BUCKET\n",
      "- name: MODEL\n",
      "- name: DOCKER_IMAGE\n",
      "- name: DOCKER_IMAGE_GPU\n",
      "- name: STAGING_FOLDER_NAME\n",
      "- name: OUTPUT_FILE_NAME\n",
      "docker:\n",
      "  imageName: gcr.io/deepvariant-docker/deepvariant_runner\n",
      "  cmd: |\n",
      "    ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \\\n",
      "      --project \"${PROJECT_ID}\" \\\n",
      "      --zones us-west1-b us-east1-d \\\n",
      "      --docker_image \"${DOCKER_IMAGE}\" \\\n",
      "      --docker_image_gpu \"${DOCKER_IMAGE_GPU}\" \\\n",
      "      --gpu \\\n",
      "      --outfile \"${OUTPUT_BUCKET}\"/\"${OUTPUT_FILE_NAME}\" \\\n",
      "      --staging \"${OUTPUT_BUCKET}\"/\"${STAGING_FOLDER_NAME}\" \\\n",
      "      --model \"${MODEL}\" \\\n",
      "      --bam gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam \\\n",
      "      --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \\\n",
      "      --shards 1024 \\\n",
      "      --make_examples_workers 16 \\\n",
      "      --make_examples_cores_per_worker 64 \\\n",
      "      --make_examples_ram_per_worker_gb 240 \\\n",
      "      --make_examples_disk_per_worker_gb 200 \\\n",
      "      --call_variants_workers 16 \\\n",
      "      --call_variants_cores_per_worker 8 \\\n",
      "      --call_variants_ram_per_worker_gb 30 \\\n",
      "      --call_variants_disk_per_worker_gb 50\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat deepvariant_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gH-orxUSVw_5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/EKr5pqmbLBiPuZyKls7ViugBIJTo_PbfBioPcHJvZHVjdGlvblF1ZXVl].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./vdv.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "BX7tMQiWVw_9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "metadata:\n",
      "  events:\n",
      "  - description: start\n",
      "    startTime: '2018-02-20T21:41:30.806641101Z'\n",
      "  - description: pulling-image\n",
      "    startTime: '2018-02-20T21:41:30.806683911Z'\n",
      "  - description: localizing-files\n",
      "    startTime: '2018-02-20T21:41:53.236795357Z'\n",
      "  - description: running-docker\n",
      "    startTime: '2018-02-20T21:41:53.236827961Z'\n",
      "  - description: delocalizing-files\n",
      "    startTime: '2018-02-20T23:52:27.673265072Z'\n",
      "  - description: ok\n",
      "    startTime: '2018-02-20T23:52:29.096456825Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "gcloud alpha genomics operations describe EKr5pqmbLBiPuZyKls7ViugBIJTo_PbfBioPcHJvZHVjdGlvblF1ZXVl  --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "kwxF9aK9VxAA"
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "gcloud alpha genomics operations cancel ELStlJCOLBiYuoPUosKa1wEg4r3yw4IMKg9wcm9kdWN0aW9uUXVldWU  --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "35AZLqwZVxAD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://genomics-labs/DV/output.vcf\n",
      "gs://genomics-labs/DV/runner_logs/\n",
      "gs://genomics-labs/DV/stage16214/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://genomics-labs/DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "bqFPdVLHVxAG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "CUKjMBkYVxAI"
   },
   "source": [
    "# GWAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ADoTxI-1VxAJ"
   },
   "source": [
    "## Genomics Tertiary analysis on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset genomics-labs:GATK_Analysis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalab.bigquery as bq\n",
    "variants_ds = bq.Dataset('GATK_Analysis')\n",
    "variants_ds.create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/notebooks/Genomics/GATK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### We now export variants from GATK4 outputs bucket and stream into big query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vcf_to_bigquery.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile vcf_to_bigquery.yaml\n",
    "name: vcf-to-bigquery-pipeline \n",
    "docker: \n",
    "  imageName: gcr.io/gcp-variant-transforms/gcp-variant-transforms \n",
    "  cmd: | \n",
    "      ./opt/gcp_variant_transforms/bin/vcf_to_bq \\\n",
    "      --project genomics-labs \\\n",
    "      --input_pattern gs://genomics-labs/gatk4/outputs/*.vcf.gz \\\n",
    "      --output_table genomics-labs:GATK_Analysis.GATKtable \\\n",
    "      --staging_location gs://genomics-labs/gatk4/staging \\\n",
    "      --temp_location gs://genomics-labs/gatk4/temp \\\n",
    "      --job_name vcf-to-bigquery \\\n",
    "      --variant_merge_strategy MOVE_TO_CALLS \\\n",
    "      --runner DataflowRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/ELny6IecLBjIwo-t8cPH_84BIJTo_PbfBioPcHJvZHVjdGlvblF1ZXVl].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud alpha genomics pipelines run \\\n",
    "    --project genomics-labs \\\n",
    "    --pipeline-file vcf_to_bigquery.yaml \\\n",
    "    --logging gs://genomics-labs/gatk4/temp/runner_logs \\\n",
    "    --zones us-east1-c \\\n",
    "    --service-account-scopes https://www.googleapis.com/auth/bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: true\n",
      "metadata:\n",
      "  events:\n",
      "  - description: start\n",
      "    startTime: '2018-02-23T04:44:20.747665848Z'\n",
      "  - description: pulling-image\n",
      "    startTime: '2018-02-23T04:44:20.748311200Z'\n",
      "  - description: localizing-files\n",
      "    startTime: '2018-02-23T04:45:17.416764804Z'\n",
      "  - description: running-docker\n",
      "    startTime: '2018-02-23T04:45:17.416798125Z'\n",
      "  - description: delocalizing-files\n",
      "    startTime: '2018-02-23T04:53:51.101151908Z'\n",
      "  - description: ok\n",
      "    startTime: '2018-02-23T04:53:52.474059467Z'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud alpha genomics operations describe EIfH5oecLBj2wsKrmK6B9n4glOj89t8GKg9wcm9kdWN0aW9uUXVldWU  --format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqsv\" id=\"1_151936192510\"></div>\n",
       "\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "      });\n",
       "\n",
       "      require(['datalab/bigquery', 'datalab/element!1_151936192510',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/bigquery.css'],\n",
       "        function(bq, dom) {\n",
       "          bq.renderSchema(dom, [{\"description\": \"Reference name.\", \"type\": \"STRING\", \"name\": \"reference_name\", \"mode\": \"NULLABLE\"}, {\"description\": \"Start position (0-based). Corresponds to the first base of the string of reference bases.\", \"type\": \"INTEGER\", \"name\": \"start_position\", \"mode\": \"NULLABLE\"}, {\"description\": \"End position (0-based). Corresponds to the first base after the last base in the reference allele.\", \"type\": \"INTEGER\", \"name\": \"end_position\", \"mode\": \"NULLABLE\"}, {\"description\": \"Reference bases.\", \"type\": \"STRING\", \"name\": \"reference_bases\", \"mode\": \"NULLABLE\"}, {\"fields\": [{\"description\": \"Alternate base.\", \"type\": \"STRING\", \"name\": \"alt\", \"mode\": \"NULLABLE\"}, {\"description\": \"Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed\", \"type\": \"INTEGER\", \"name\": \"MLEAC\", \"mode\": \"NULLABLE\"}, {\"description\": \"Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed\", \"type\": \"FLOAT\", \"name\": \"MLEAF\", \"mode\": \"NULLABLE\"}], \"description\": \"One record for each alternate base (if any).\", \"type\": \"RECORD\", \"name\": \"alternate_bases\", \"mode\": \"REPEATED\"}, {\"description\": \"Variant names (e.g. RefSNP ID).\", \"type\": \"STRING\", \"name\": \"names\", \"mode\": \"REPEATED\"}, {\"description\": \"Phred-scaled quality score (-10log10 prob(call is wrong)). Higher values imply better quality.\", \"type\": \"FLOAT\", \"name\": \"quality\", \"mode\": \"NULLABLE\"}, {\"description\": \"List of failed filters (if any) or \\\"PASS\\\" indicating the variant has passed all filters.\", \"type\": \"STRING\", \"name\": \"filter\", \"mode\": \"REPEATED\"}, {\"fields\": [{\"description\": \"Name of the call.\", \"type\": \"STRING\", \"name\": \"name\", \"mode\": \"NULLABLE\"}, {\"description\": \"Genotype of the call. \\\"-1\\\" is used in cases where the genotype is not called.\", \"type\": \"INTEGER\", \"name\": \"genotype\", \"mode\": \"REPEATED\"}, {\"description\": \"Phaseset of the call (if any). \\\"*\\\" is used in cases where the genotype is phased, but no phase set (\\\"PS\\\" in FORMAT) was specified.\", \"type\": \"STRING\", \"name\": \"phaseset\", \"mode\": \"NULLABLE\"}, {\"description\": \"Minimum DP observed within the GVCF block\", \"type\": \"INTEGER\", \"name\": \"MIN_DP\", \"mode\": \"NULLABLE\"}, {\"description\": \"Allelic depths for the ref and alt alleles in the order listed\", \"type\": \"INTEGER\", \"name\": \"AD\", \"mode\": \"REPEATED\"}, {\"description\": \"Genotype Quality\", \"type\": \"INTEGER\", \"name\": \"GQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group\", \"type\": \"STRING\", \"name\": \"PID\", \"mode\": \"NULLABLE\"}, {\"description\": \"Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another\", \"type\": \"STRING\", \"name\": \"PGT\", \"mode\": \"NULLABLE\"}, {\"description\": \"Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification\", \"type\": \"INTEGER\", \"name\": \"PL\", \"mode\": \"REPEATED\"}, {\"description\": \"Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.\", \"type\": \"INTEGER\", \"name\": \"SB\", \"mode\": \"REPEATED\"}, {\"description\": \"Approximate read depth (reads with MQ=255 or with bad mates are filtered)\", \"type\": \"INTEGER\", \"name\": \"DP\", \"mode\": \"NULLABLE\"}], \"description\": \"One record for each call.\", \"type\": \"RECORD\", \"name\": \"call\", \"mode\": \"REPEATED\"}, {\"description\": \"Phred-scaled p-value for exact test of excess heterozygosity\", \"type\": \"FLOAT\", \"name\": \"ExcessHet\", \"mode\": \"NULLABLE\"}, {\"description\": \"Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities\", \"type\": \"FLOAT\", \"name\": \"BaseQRankSum\", \"mode\": \"NULLABLE\"}, {\"description\": \"Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities\", \"type\": \"FLOAT\", \"name\": \"MQRankSum\", \"mode\": \"NULLABLE\"}, {\"description\": \"Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation\", \"type\": \"FLOAT\", \"name\": \"InbreedingCoeff\", \"mode\": \"NULLABLE\"}, {\"description\": \"RMS Mapping Quality\", \"type\": \"FLOAT\", \"name\": \"MQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"Raw data for RMS Mapping Quality\", \"type\": \"FLOAT\", \"name\": \"RAW_MQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"Consistency of the site with at most two segregating haplotypes\", \"type\": \"FLOAT\", \"name\": \"HaplotypeScore\", \"mode\": \"NULLABLE\"}, {\"description\": \"Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases\", \"type\": \"FLOAT\", \"name\": \"ClippingRankSum\", \"mode\": \"NULLABLE\"}, {\"description\": \"Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias\", \"type\": \"FLOAT\", \"name\": \"ReadPosRankSum\", \"mode\": \"NULLABLE\"}, {\"description\": \"Were any of the samples downsampled?\", \"type\": \"BOOLEAN\", \"name\": \"DS\", \"mode\": \"NULLABLE\"}, {\"description\": \"Approximate read depth; some reads may have been filtered\", \"type\": \"INTEGER\", \"name\": \"DP\", \"mode\": \"NULLABLE\"}]);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "[{ 'name': 'reference_name', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Reference name.' },\n",
       " { 'name': 'start_position', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Start position (0-based). Corresponds to the first base of the string of reference bases.' },\n",
       " { 'name': 'end_position', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'End position (0-based). Corresponds to the first base after the last base in the reference allele.' },\n",
       " { 'name': 'reference_bases', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Reference bases.' },\n",
       " { 'name': 'alternate_bases', 'type': 'RECORD', 'mode':'REPEATED', 'description': 'One record for each alternate base (if any).' },\n",
       " { 'name': 'alternate_bases.alt', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Alternate base.' },\n",
       " { 'name': 'alternate_bases.MLEAC', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed' },\n",
       " { 'name': 'alternate_bases.MLEAF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed' },\n",
       " { 'name': 'names', 'type': 'STRING', 'mode':'REPEATED', 'description': 'Variant names (e.g. RefSNP ID).' },\n",
       " { 'name': 'quality', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Phred-scaled quality score (-10log10 prob(call is wrong)). Higher values imply better quality.' },\n",
       " { 'name': 'filter', 'type': 'STRING', 'mode':'REPEATED', 'description': 'List of failed filters (if any) or \"PASS\" indicating the variant has passed all filters.' },\n",
       " { 'name': 'call', 'type': 'RECORD', 'mode':'REPEATED', 'description': 'One record for each call.' },\n",
       " { 'name': 'call.name', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Name of the call.' },\n",
       " { 'name': 'call.genotype', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'Genotype of the call. \"-1\" is used in cases where the genotype is not called.' },\n",
       " { 'name': 'call.phaseset', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Phaseset of the call (if any). \"*\" is used in cases where the genotype is phased, but no phase set (\"PS\" in FORMAT) was specified.' },\n",
       " { 'name': 'call.MIN_DP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Minimum DP observed within the GVCF block' },\n",
       " { 'name': 'call.AD', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'Allelic depths for the ref and alt alleles in the order listed' },\n",
       " { 'name': 'call.GQ', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Genotype Quality' },\n",
       " { 'name': 'call.PID', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group' },\n",
       " { 'name': 'call.PGT', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another' },\n",
       " { 'name': 'call.PL', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification' },\n",
       " { 'name': 'call.SB', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.' },\n",
       " { 'name': 'call.DP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Approximate read depth (reads with MQ=255 or with bad mates are filtered)' },\n",
       " { 'name': 'ExcessHet', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Phred-scaled p-value for exact test of excess heterozygosity' },\n",
       " { 'name': 'BaseQRankSum', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities' },\n",
       " { 'name': 'MQRankSum', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities' },\n",
       " { 'name': 'InbreedingCoeff', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation' },\n",
       " { 'name': 'MQ', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'RMS Mapping Quality' },\n",
       " { 'name': 'RAW_MQ', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Raw data for RMS Mapping Quality' },\n",
       " { 'name': 'HaplotypeScore', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Consistency of the site with at most two segregating haplotypes' },\n",
       " { 'name': 'ClippingRankSum', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases' },\n",
       " { 'name': 'ReadPosRankSum', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias' },\n",
       " { 'name': 'DS', 'type': 'BOOLEAN', 'mode':'NULLABLE', 'description': 'Were any of the samples downsampled?' },\n",
       " { 'name': 'DP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'Approximate read depth; some reads may have been filtered' }]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variants_table = bq.Table('genomics-labs:GATK_Analysis.GATKtable')\n",
    "variants_table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Lets count the number of records (variant and reference segments) we have in the dataset and the total number of calls nested within those records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqtv\" id=\"3_151936258983\"><table><tr><th>reference_name</th><th>num_records</th><th>num_calls</th></tr><tr><td>chr1</td><td>25280</td><td>25280</td></tr><tr><td>chr10</td><td>14651</td><td>14651</td></tr><tr><td>chr11</td><td>13933</td><td>13933</td></tr><tr><td>chr12</td><td>14009</td><td>14009</td></tr><tr><td>chr13</td><td>10490</td><td>10490</td></tr><tr><td>chr14</td><td>8754</td><td>8754</td></tr><tr><td>chr15</td><td>8532</td><td>8532</td></tr><tr><td>chr16</td><td>9685</td><td>9685</td></tr><tr><td>chr17</td><td>8554</td><td>8554</td></tr><tr><td>chr18</td><td>8420</td><td>8420</td></tr><tr><td>chr19</td><td>6794</td><td>6794</td></tr><tr><td>chr2</td><td>24813</td><td>24813</td></tr><tr><td>chr20</td><td>6913</td><td>6913</td></tr><tr><td>chr21</td><td>4311</td><td>4311</td></tr><tr><td>chr22</td><td>4540</td><td>4540</td></tr><tr><td>chr3</td><td>22073</td><td>22073</td></tr><tr><td>chr4</td><td>20450</td><td>20450</td></tr><tr><td>chr5</td><td>17259</td><td>17259</td></tr><tr><td>chr6</td><td>17350</td><td>17350</td></tr><tr><td>chr7</td><td>15234</td><td>15234</td></tr><tr><td>chr8</td><td>14614</td><td>14614</td></tr><tr><td>chr9</td><td>11691</td><td>11691</td></tr><tr><td>chrX</td><td>14310</td><td>14310</td></tr><tr><td>chrY</td><td>342</td><td>342</td></tr></table></div>\n",
       "    <br />(rows: 24, time: 0.3s, cached, job: job_USe5kCrBHe0rCZTHFWzsgBVCdO1S)<br />\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "          d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3',\n",
       "          plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',\n",
       "          jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min'\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "        shim: {\n",
       "          plotly: {\n",
       "            deps: ['d3', 'jquery'],\n",
       "            exports: 'plotly'\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "\n",
       "      require(['datalab/charting', 'datalab/element!3_151936258983', 'base/js/events',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/charting.css'],\n",
       "        function(charts, dom, events) {\n",
       "          charts.render('gcharts', dom, events, 'table', [], {\"rows\": [{\"c\": [{\"v\": \"chr1\"}, {\"v\": 25280}, {\"v\": 25280}]}, {\"c\": [{\"v\": \"chr10\"}, {\"v\": 14651}, {\"v\": 14651}]}, {\"c\": [{\"v\": \"chr11\"}, {\"v\": 13933}, {\"v\": 13933}]}, {\"c\": [{\"v\": \"chr12\"}, {\"v\": 14009}, {\"v\": 14009}]}, {\"c\": [{\"v\": \"chr13\"}, {\"v\": 10490}, {\"v\": 10490}]}, {\"c\": [{\"v\": \"chr14\"}, {\"v\": 8754}, {\"v\": 8754}]}, {\"c\": [{\"v\": \"chr15\"}, {\"v\": 8532}, {\"v\": 8532}]}, {\"c\": [{\"v\": \"chr16\"}, {\"v\": 9685}, {\"v\": 9685}]}, {\"c\": [{\"v\": \"chr17\"}, {\"v\": 8554}, {\"v\": 8554}]}, {\"c\": [{\"v\": \"chr18\"}, {\"v\": 8420}, {\"v\": 8420}]}, {\"c\": [{\"v\": \"chr19\"}, {\"v\": 6794}, {\"v\": 6794}]}, {\"c\": [{\"v\": \"chr2\"}, {\"v\": 24813}, {\"v\": 24813}]}, {\"c\": [{\"v\": \"chr20\"}, {\"v\": 6913}, {\"v\": 6913}]}, {\"c\": [{\"v\": \"chr21\"}, {\"v\": 4311}, {\"v\": 4311}]}, {\"c\": [{\"v\": \"chr22\"}, {\"v\": 4540}, {\"v\": 4540}]}, {\"c\": [{\"v\": \"chr3\"}, {\"v\": 22073}, {\"v\": 22073}]}, {\"c\": [{\"v\": \"chr4\"}, {\"v\": 20450}, {\"v\": 20450}]}, {\"c\": [{\"v\": \"chr5\"}, {\"v\": 17259}, {\"v\": 17259}]}, {\"c\": [{\"v\": \"chr6\"}, {\"v\": 17350}, {\"v\": 17350}]}, {\"c\": [{\"v\": \"chr7\"}, {\"v\": 15234}, {\"v\": 15234}]}, {\"c\": [{\"v\": \"chr8\"}, {\"v\": 14614}, {\"v\": 14614}]}, {\"c\": [{\"v\": \"chr9\"}, {\"v\": 11691}, {\"v\": 11691}]}, {\"c\": [{\"v\": \"chrX\"}, {\"v\": 14310}, {\"v\": 14310}]}, {\"c\": [{\"v\": \"chrY\"}, {\"v\": 342}, {\"v\": 342}]}], \"cols\": [{\"type\": \"string\", \"id\": \"reference_name\", \"label\": \"reference_name\"}, {\"type\": \"number\", \"id\": \"num_records\", \"label\": \"num_records\"}, {\"type\": \"number\", \"id\": \"num_calls\", \"label\": \"num_calls\"}]},\n",
       "            {\n",
       "              pageSize: 25,\n",
       "              cssClassNames:  {\n",
       "                tableRow: 'gchart-table-row',\n",
       "                headerRow: 'gchart-table-headerrow',\n",
       "                oddTableRow: 'gchart-table-oddrow',\n",
       "                selectedTableRow: 'gchart-table-selectedrow',\n",
       "                hoverTableRow: 'gchart-table-hoverrow',\n",
       "                tableCell: 'gchart-table-cell',\n",
       "                headerCell: 'gchart-table-headercell',\n",
       "                rowNumberCell: 'gchart-table-rownumcell'\n",
       "              }\n",
       "            },\n",
       "            {source_index: 0, fields: 'reference_name,num_records,num_calls'},\n",
       "            0,\n",
       "            24);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "  "
      ],
      "text/plain": [
       "QueryResultsTable job_USe5kCrBHe0rCZTHFWzsgBVCdO1S"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bq query\n",
    "\n",
    "SELECT\n",
    "  reference_name,\n",
    "  COUNT(reference_name) AS num_records,\n",
    "  SUM(ARRAY_LENGTH(call)) AS num_calls\n",
    "FROM\n",
    "  `genomics-labs.GATK_Analysis.GATKtable`\n",
    "GROUP BY\n",
    "  reference_name\n",
    "ORDER BY\n",
    "  reference_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pR3OILIuVxAK"
   },
   "source": [
    "### Design\n",
    "\n",
    "We'll be replicating a clinical study: Control Group and Test Group. \n",
    "One will be the reference and the other will the phenotype carrier (plug in here any pathology worth investigating at the Genetic level)\n",
    "\n",
    "Our goal is to filter through BILLIONS of variants and pnpoint only the variant calls (for the sake of time  we'll do this within one chromosome, let's say the #12) that differs significantly between the case and control groups. If we can identify and isolate that we'll have a good chance of identifying a 'genenotype phenotype causality' (in other words identify what a the genetic level creates the pathology - in real life/research is not that easy but this is just a demo)\n",
    "\n",
    "The case group for the purposes of this notebook will be individuals from the \"EAS\" (East Asian) super population. Variant data from the 1000 genomes dataset is publicly accessible within BigQuery.\n",
    "\n",
    "We'll make the EAS group our test group and use the rest of the 1000 genomes as the control group.\n",
    "\n",
    "We'll compare them and isolate the most meaningful differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "lY2AIqxrVxAK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqsv\" id=\"1_151924966143\"></div>\n",
       "\n",
       "    <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "    <script>\n",
       "      require.config({\n",
       "        paths: {\n",
       "          base: '/static/base',\n",
       "        },\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "      });\n",
       "\n",
       "      require(['datalab/bigquery', 'datalab/element!1_151924966143',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/bigquery.css'],\n",
       "        function(bq, dom) {\n",
       "          bq.renderSchema(dom, [{\"description\": \"An identifier from the reference genome or an angle-bracketed ID String pointing to a contig in the assembly file.\", \"type\": \"STRING\", \"name\": \"reference_name\", \"mode\": \"NULLABLE\"}, {\"description\": \"The reference position, with the first base having position 0.\", \"type\": \"INTEGER\", \"name\": \"start\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=END,Number=1,Type=Integer,Description=\\\"End position of the variant described in this record\\\">\", \"type\": \"INTEGER\", \"name\": \"end\", \"mode\": \"NULLABLE\"}, {\"description\": \"Each base must be one of A,C,G,T,N (case insensitive). Multiple bases are permitted. The value in the POS field refers to the position of the first base in the String.\", \"type\": \"STRING\", \"name\": \"reference_bases\", \"mode\": \"NULLABLE\"}, {\"description\": \"List of alternate non-reference alleles called on at least one of the samples. (\\\"at least one\\\" not true for this dataset)\", \"type\": \"STRING\", \"name\": \"alternate_bases\", \"mode\": \"REPEATED\"}, {\"description\": \"phred-scaled quality score for the assertion made in ALT.\", \"type\": \"FLOAT\", \"name\": \"quality\", \"mode\": \"NULLABLE\"}, {\"description\": \"PASS if this position has passed all filters, i.e. a call is made at this position. Otherwise, if the site has not passed all filters, a list of codes for filters that fail.\", \"type\": \"STRING\", \"name\": \"filter\", \"mode\": \"REPEATED\"}, {\"description\": \"List of unique identifiers for the variant where available.\", \"type\": \"STRING\", \"name\": \"names\", \"mode\": \"REPEATED\"}, {\"fields\": [{\"description\": \"The id of the callset from which this data was exported from the Google Genomics Variants API.\", \"type\": \"STRING\", \"name\": \"call_set_id\", \"mode\": \"NULLABLE\"}, {\"description\": \"Sample identifier.\", \"type\": \"STRING\", \"name\": \"call_set_name\", \"mode\": \"NULLABLE\"}, {\"description\": \"List of genotypes.\", \"type\": \"INTEGER\", \"name\": \"genotype\", \"mode\": \"REPEATED\"}, {\"description\": \"If this value is null, the data is unphased.  Otherwise it is phased.\", \"type\": \"STRING\", \"name\": \"phaseset\", \"mode\": \"NULLABLE\"}, {\"description\": \"List of genotype likelihoods.\", \"type\": \"FLOAT\", \"name\": \"genotype_likelihood\", \"mode\": \"REPEATED\"}, {\"description\": \"FORMAT=<ID=DP,Number=1,Type=Integer,Description=\\\"# high-quality bases\\\">\", \"type\": \"INTEGER\", \"name\": \"DP\", \"mode\": \"NULLABLE\"}, {\"description\": \"FORMAT=<ID=DS,Number=1,Type=Float,Description=\\\"Genotype dosage from MaCH/Thunder\\\">\", \"type\": \"FLOAT\", \"name\": \"DS\", \"mode\": \"NULLABLE\"}, {\"type\": \"STRING\", \"name\": \"FT\", \"mode\": \"NULLABLE\"}, {\"description\": \"FORMAT=<ID=GQ,Number=1,Type=Float,Description=\\\"Genotype quality\\\">\", \"type\": \"STRING\", \"name\": \"GQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"FORMAT=<ID=PL,Number=.,Type=Integer,Description=\\\"List of Phred-scaled genotype likelihoods, number of values is (#ALT+1)*(#ALT+2)/2\\\">\", \"type\": \"INTEGER\", \"name\": \"PL\", \"mode\": \"REPEATED\"}, {\"description\": \"FORMAT=<ID=SP,Number=1,Type=Integer,Description=\\\"Phred-scaled strand bias P-value\\\">\", \"type\": \"INTEGER\", \"name\": \"SP\", \"mode\": \"NULLABLE\"}], \"description\": \"Per-sample measurements.\", \"type\": \"RECORD\", \"name\": \"call\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=AA,Number=1,Type=String,Description=\\\"Ancestral Allele, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/pilot_data/technical/reference/ancestral_alignments/README\\\">\", \"type\": \"STRING\", \"name\": \"AA\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AC,Number=.,Type=Integer,Description=\\\"Alternate Allele Count\\\">\", \"type\": \"INTEGER\", \"name\": \"AC\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=AC1,Number=1,Type=Float,Description=\\\"Max-likelihood estimate of the first ALT allele count (no HWE assumption)\\\">\", \"type\": \"INTEGER\", \"name\": \"AC1\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AF,Number=1,Type=Float,Description=\\\"Global Allele Frequency based on AC/AN\\\">\", \"type\": \"FLOAT\", \"name\": \"AF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AF1,Number=1,Type=Float,Description=\\\"Max-likelihood estimate of the first ALT allele frequency (assuming HWE)\\\">\", \"type\": \"FLOAT\", \"name\": \"AF1\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AFR_AF,Number=1,Type=Float,Description=\\\"Allele Frequency for samples from AFR based on AC/AN\\\">\", \"type\": \"FLOAT\", \"name\": \"AFR_AF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AMR_AF,Number=1,Type=Float,Description=\\\"Allele Frequency for samples from AMR based on AC/AN\\\">\", \"type\": \"FLOAT\", \"name\": \"AMR_AF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AN,Number=1,Type=Integer,Description=\\\"Total Allele Count\\\">\", \"type\": \"INTEGER\", \"name\": \"AN\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=ASN_AF,Number=1,Type=Float,Description=\\\"Allele Frequency for samples from ASN based on AC/AN\\\">\", \"type\": \"FLOAT\", \"name\": \"ASN_AF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=AVGPOST,Number=1,Type=Float,Description=\\\"Average posterior probability from MaCH/Thunder\\\">\", \"type\": \"FLOAT\", \"name\": \"AVGPOST\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=CIEND,Number=2,Type=Integer,Description=\\\"Confidence interval around END for imprecise variants\\\">\", \"type\": \"INTEGER\", \"name\": \"CIEND\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=CIPOS,Number=2,Type=Integer,Description=\\\"Confidence interval around POS for imprecise variants\\\">\", \"type\": \"INTEGER\", \"name\": \"CIPOS\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=DP,Number=1,Type=Integer,Description=\\\"Raw read depth\\\">\", \"type\": \"INTEGER\", \"name\": \"DP\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=DP4,Number=4,Type=Integer,Description=\\\"# high-quality ref-forward bases, ref-reverse, alt-forward and alt-reverse bases\\\">\", \"type\": \"INTEGER\", \"name\": \"DP4\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=ERATE,Number=1,Type=Float,Description=\\\"Per-marker Mutation rate from MaCH/Thunder\\\">\", \"type\": \"FLOAT\", \"name\": \"ERATE\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=EUR_AF,Number=1,Type=Float,Description=\\\"Allele Frequency for samples from EUR based on AC/AN\\\">\", \"type\": \"FLOAT\", \"name\": \"EUR_AF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=FQ,Number=1,Type=Float,Description=\\\"Phred probability of all samples being the same\\\">\", \"type\": \"FLOAT\", \"name\": \"FQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=G3,Number=3,Type=Float,Description=\\\"ML estimate of genotype frequencies\\\">\", \"type\": \"FLOAT\", \"name\": \"G3\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=HOMLEN,Number=.,Type=Integer,Description=\\\"Length of base pair identical micro-homology at event breakpoints\\\">\", \"type\": \"INTEGER\", \"name\": \"HOMLEN\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=HOMSEQ,Number=.,Type=String,Description=\\\"Sequence of base pair identical micro-homology at event breakpoints\\\">\", \"type\": \"STRING\", \"name\": \"HOMSEQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=HWE,Number=1,Type=Float,Description=\\\"Chi^2 based HWE test P-value based on G3\\\">\", \"type\": \"FLOAT\", \"name\": \"HWE\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=LDAF,Number=1,Type=Float,Description=\\\"MLE Allele Frequency Accounting for LD\\\">\", \"type\": \"FLOAT\", \"name\": \"LDAF\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=MQ,Number=1,Type=Integer,Description=\\\"Root-mean-square mapping quality of covering reads\\\">\", \"type\": \"INTEGER\", \"name\": \"MQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=PV4,Number=4,Type=Float,Description=\\\"P-values for strand bias, baseQ bias, mapQ bias and tail distance bias\\\">\", \"type\": \"FLOAT\", \"name\": \"PV4\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=RSQ,Number=1,Type=Float,Description=\\\"Genotype imputation quality from MaCH/Thunder\\\">\", \"type\": \"FLOAT\", \"name\": \"RSQ\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=SNPSOURCE,Number=.,Type=String,Description=\\\"indicates if a snp was called when analysing the low coverage or exome alignment data\\\">\", \"type\": \"STRING\", \"name\": \"SNPSOURCE\", \"mode\": \"REPEATED\"}, {\"description\": \"\", \"type\": \"STRING\", \"name\": \"SOURCE\", \"mode\": \"REPEATED\"}, {\"description\": \"INFO=<ID=SVLEN,Number=1,Type=Integer,Description=\\\"Difference in length between REF and ALT alleles\\\">\", \"type\": \"INTEGER\", \"name\": \"SVLEN\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=SVTYPE,Number=1,Type=String,Description=\\\"Type of structural variant\\\">\", \"type\": \"STRING\", \"name\": \"SVTYPE\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=THETA,Number=1,Type=Float,Description=\\\"Per-marker Transition rate from MaCH/Thunder\\\">\", \"type\": \"FLOAT\", \"name\": \"THETA\", \"mode\": \"NULLABLE\"}, {\"description\": \"INFO=<ID=VT,Number=1,Type=String,Description=\\\"indicates what type of variant the line represents\\\">\", \"type\": \"STRING\", \"name\": \"VT\", \"mode\": \"NULLABLE\"}]);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "[{ 'name': 'reference_name', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'An identifier from the reference genome or an angle-bracketed ID String pointing to a contig in the assembly file.' },\n",
       " { 'name': 'start', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'The reference position, with the first base having position 0.' },\n",
       " { 'name': 'end', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the variant described in this record\">' },\n",
       " { 'name': 'reference_bases', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Each base must be one of A,C,G,T,N (case insensitive). Multiple bases are permitted. The value in the POS field refers to the position of the first base in the String.' },\n",
       " { 'name': 'alternate_bases', 'type': 'STRING', 'mode':'REPEATED', 'description': 'List of alternate non-reference alleles called on at least one of the samples. (\"at least one\" not true for this dataset)' },\n",
       " { 'name': 'quality', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'phred-scaled quality score for the assertion made in ALT.' },\n",
       " { 'name': 'filter', 'type': 'STRING', 'mode':'REPEATED', 'description': 'PASS if this position has passed all filters, i.e. a call is made at this position. Otherwise, if the site has not passed all filters, a list of codes for filters that fail.' },\n",
       " { 'name': 'names', 'type': 'STRING', 'mode':'REPEATED', 'description': 'List of unique identifiers for the variant where available.' },\n",
       " { 'name': 'call', 'type': 'RECORD', 'mode':'REPEATED', 'description': 'Per-sample measurements.' },\n",
       " { 'name': 'call.call_set_id', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'The id of the callset from which this data was exported from the Google Genomics Variants API.' },\n",
       " { 'name': 'call.call_set_name', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'Sample identifier.' },\n",
       " { 'name': 'call.genotype', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'List of genotypes.' },\n",
       " { 'name': 'call.phaseset', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'If this value is null, the data is unphased.  Otherwise it is phased.' },\n",
       " { 'name': 'call.genotype_likelihood', 'type': 'FLOAT', 'mode':'REPEATED', 'description': 'List of genotype likelihoods.' },\n",
       " { 'name': 'call.DP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"# high-quality bases\">' },\n",
       " { 'name': 'call.DS', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'FORMAT=<ID=DS,Number=1,Type=Float,Description=\"Genotype dosage from MaCH/Thunder\">' },\n",
       " { 'name': 'call.FT', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'None' },\n",
       " { 'name': 'call.GQ', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'FORMAT=<ID=GQ,Number=1,Type=Float,Description=\"Genotype quality\">' },\n",
       " { 'name': 'call.PL', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'FORMAT=<ID=PL,Number=.,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods, number of values is (#ALT+1)*(#ALT+2)/2\">' },\n",
       " { 'name': 'call.SP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'FORMAT=<ID=SP,Number=1,Type=Integer,Description=\"Phred-scaled strand bias P-value\">' },\n",
       " { 'name': 'AA', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele, ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/pilot_data/technical/reference/ancestral_alignments/README\">' },\n",
       " { 'name': 'AC', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'INFO=<ID=AC,Number=.,Type=Integer,Description=\"Alternate Allele Count\">' },\n",
       " { 'name': 'AC1', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=AC1,Number=1,Type=Float,Description=\"Max-likelihood estimate of the first ALT allele count (no HWE assumption)\">' },\n",
       " { 'name': 'AF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=AF,Number=1,Type=Float,Description=\"Global Allele Frequency based on AC/AN\">' },\n",
       " { 'name': 'AF1', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=AF1,Number=1,Type=Float,Description=\"Max-likelihood estimate of the first ALT allele frequency (assuming HWE)\">' },\n",
       " { 'name': 'AFR_AF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=AFR_AF,Number=1,Type=Float,Description=\"Allele Frequency for samples from AFR based on AC/AN\">' },\n",
       " { 'name': 'AMR_AF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=AMR_AF,Number=1,Type=Float,Description=\"Allele Frequency for samples from AMR based on AC/AN\">' },\n",
       " { 'name': 'AN', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total Allele Count\">' },\n",
       " { 'name': 'ASN_AF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=ASN_AF,Number=1,Type=Float,Description=\"Allele Frequency for samples from ASN based on AC/AN\">' },\n",
       " { 'name': 'AVGPOST', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=AVGPOST,Number=1,Type=Float,Description=\"Average posterior probability from MaCH/Thunder\">' },\n",
       " { 'name': 'CIEND', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'INFO=<ID=CIEND,Number=2,Type=Integer,Description=\"Confidence interval around END for imprecise variants\">' },\n",
       " { 'name': 'CIPOS', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'INFO=<ID=CIPOS,Number=2,Type=Integer,Description=\"Confidence interval around POS for imprecise variants\">' },\n",
       " { 'name': 'DP', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\">' },\n",
       " { 'name': 'DP4', 'type': 'INTEGER', 'mode':'REPEATED', 'description': 'INFO=<ID=DP4,Number=4,Type=Integer,Description=\"# high-quality ref-forward bases, ref-reverse, alt-forward and alt-reverse bases\">' },\n",
       " { 'name': 'ERATE', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=ERATE,Number=1,Type=Float,Description=\"Per-marker Mutation rate from MaCH/Thunder\">' },\n",
       " { 'name': 'EUR_AF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=EUR_AF,Number=1,Type=Float,Description=\"Allele Frequency for samples from EUR based on AC/AN\">' },\n",
       " { 'name': 'FQ', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=FQ,Number=1,Type=Float,Description=\"Phred probability of all samples being the same\">' },\n",
       " { 'name': 'G3', 'type': 'FLOAT', 'mode':'REPEATED', 'description': 'INFO=<ID=G3,Number=3,Type=Float,Description=\"ML estimate of genotype frequencies\">' },\n",
       " { 'name': 'HOMLEN', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=HOMLEN,Number=.,Type=Integer,Description=\"Length of base pair identical micro-homology at event breakpoints\">' },\n",
       " { 'name': 'HOMSEQ', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'INFO=<ID=HOMSEQ,Number=.,Type=String,Description=\"Sequence of base pair identical micro-homology at event breakpoints\">' },\n",
       " { 'name': 'HWE', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=HWE,Number=1,Type=Float,Description=\"Chi^2 based HWE test P-value based on G3\">' },\n",
       " { 'name': 'LDAF', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=LDAF,Number=1,Type=Float,Description=\"MLE Allele Frequency Accounting for LD\">' },\n",
       " { 'name': 'MQ', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=MQ,Number=1,Type=Integer,Description=\"Root-mean-square mapping quality of covering reads\">' },\n",
       " { 'name': 'PV4', 'type': 'FLOAT', 'mode':'REPEATED', 'description': 'INFO=<ID=PV4,Number=4,Type=Float,Description=\"P-values for strand bias, baseQ bias, mapQ bias and tail distance bias\">' },\n",
       " { 'name': 'RSQ', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=RSQ,Number=1,Type=Float,Description=\"Genotype imputation quality from MaCH/Thunder\">' },\n",
       " { 'name': 'SNPSOURCE', 'type': 'STRING', 'mode':'REPEATED', 'description': 'INFO=<ID=SNPSOURCE,Number=.,Type=String,Description=\"indicates if a snp was called when analysing the low coverage or exome alignment data\">' },\n",
       " { 'name': 'SOURCE', 'type': 'STRING', 'mode':'REPEATED', 'description': '' },\n",
       " { 'name': 'SVLEN', 'type': 'INTEGER', 'mode':'NULLABLE', 'description': 'INFO=<ID=SVLEN,Number=1,Type=Integer,Description=\"Difference in length between REF and ALT alleles\">' },\n",
       " { 'name': 'SVTYPE', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'INFO=<ID=SVTYPE,Number=1,Type=String,Description=\"Type of structural variant\">' },\n",
       " { 'name': 'THETA', 'type': 'FLOAT', 'mode':'NULLABLE', 'description': 'INFO=<ID=THETA,Number=1,Type=Float,Description=\"Per-marker Transition rate from MaCH/Thunder\">' },\n",
       " { 'name': 'VT', 'type': 'STRING', 'mode':'NULLABLE', 'description': 'INFO=<ID=VT,Number=1,Type=String,Description=\"indicates what type of variant the line represents\">' }]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalab.bigquery as bq\n",
    "variants_table = bq.Table('genomics-public-data:1000_genomes.variants')\n",
    "variants_table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "BRGmlOMTVxAO"
   },
   "source": [
    "### Classifying per-call variant positions into variant/non-variant groups\n",
    "\n",
    "We can SUM the reference/alternate allele accounts for individual variant positions within chromosome 12.\n",
    "\n",
    "The field call.genotype is an integer ranging from [-1, num_alternate_bases].\n",
    "\n",
    "A value of negative one indicates that the genotype for the call is ambiguous (i.e., a no-call). A value of zero indicates that the genotype for the call is the same as the reference (i.e., non-variant). A value of one would indicate that the genotype for the call is the 1st value in the list of alternate bases (likewise for values >1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "G7_C6HJTVxAO"
   },
   "outputs": [],
   "source": [
    "%%sql --module allele_counts\n",
    "\n",
    "SELECT \n",
    "    reference_name,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    end,\n",
    "    vt,    \n",
    "    call.call_set_name AS call_set_name,\n",
    "    # 1000 genomes phase 1 data is bi-allelic so there is only ever a single alt\n",
    "    SUM(0 = call.genotype) WITHIN RECORD AS ref_count,\n",
    "    SUM(1 = call.genotype) WITHIN RECORD AS alt_count,\n",
    "FROM\n",
    "  FLATTEN((\n",
    "    SELECT\n",
    "      reference_name,\n",
    "      start,\n",
    "      reference_bases,\n",
    "      alternate_bases,\n",
    "      end,\n",
    "      vt,\n",
    "      call.call_set_name,\n",
    "      call.genotype,\n",
    "    FROM\n",
    "      $variants_table\n",
    "    WHERE\n",
    "      reference_name = '12' -- i.e., chromosome 12\n",
    "    ),\n",
    "    call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "tc_zM6MPVxAR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_name</th>\n",
       "      <th>start</th>\n",
       "      <th>reference_bases</th>\n",
       "      <th>alternate_bases</th>\n",
       "      <th>end</th>\n",
       "      <th>vt</th>\n",
       "      <th>call_set_name</th>\n",
       "      <th>ref_count</th>\n",
       "      <th>alt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>54319726</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>54319727</td>\n",
       "      <td>SNP</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>54319726</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>54319727</td>\n",
       "      <td>SNP</td>\n",
       "      <td>HG00593</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>54319726</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>54319727</td>\n",
       "      <td>SNP</td>\n",
       "      <td>NA12749</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>54319726</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>54319727</td>\n",
       "      <td>SNP</td>\n",
       "      <td>HG00150</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>54319726</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>54319727</td>\n",
       "      <td>SNP</td>\n",
       "      <td>NA19675</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reference_name     start reference_bases alternate_bases       end   vt  \\\n",
       "0             12  54319726               A               G  54319727  SNP   \n",
       "1             12  54319726               A               G  54319727  SNP   \n",
       "2             12  54319726               A               G  54319727  SNP   \n",
       "3             12  54319726               A               G  54319727  SNP   \n",
       "4             12  54319726               A               G  54319727  SNP   \n",
       "\n",
       "  call_set_name  ref_count  alt_count  \n",
       "0       HG00261          0          2  \n",
       "1       HG00593          0          2  \n",
       "2       NA12749          1          1  \n",
       "3       HG00150          0          2  \n",
       "4       NA19675          0          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq.Query(allele_counts, variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "yVTsiFOrVxAV"
   },
   "source": [
    "### Assigning case and control groups\n",
    "\n",
    "We can join our allele counts with metadata available in the sample info table.\n",
    "\n",
    "Use this sample metadata to split the set of genomes into case and control groups based upon the super population group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VbTUEmZfVxAV"
   },
   "outputs": [],
   "source": [
    "%%sql --module exp_groups\n",
    "\n",
    "SELECT\n",
    "  super_population,\n",
    "  ('EAS' = super_population) AS is_case,\n",
    "  call_set_name,\n",
    "  reference_name,\n",
    "  start,\n",
    "  reference_bases,\n",
    "  alternate_bases,\n",
    "  end,\n",
    "  vt,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "FROM $allele_counts AS allele_counts\n",
    "JOIN $sample_info_table AS samples\n",
    "  ON allele_counts.call_set_name = samples.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "OxlOKlNJVxAY"
   },
   "outputs": [],
   "source": [
    "bq.Query(exp_groups, allele_counts=allele_counts,\n",
    "                     sample_info_table=sample_info_table,\n",
    "                     variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "vpoKfh2PVxAb"
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT \n",
    "  vt,\n",
    "  COUNT(*)\n",
    "FROM $exp_groups\n",
    "GROUP BY vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Mjre-sHjVxAe"
   },
   "source": [
    "### close to TWO BILLIONS SNPs. For the purposes of this DEMO we'll  limit the variants to only SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Ge8DTH-pVxAe"
   },
   "outputs": [],
   "source": [
    "%%sql --module snps\n",
    "SELECT * \n",
    "FROM $exp_groups\n",
    "WHERE vt='SNP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "DKVKpMGZVxAh"
   },
   "outputs": [],
   "source": [
    "bq.Query(snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "QWj2gn4DVxAj"
   },
   "source": [
    "### SUMMING reference/alternate allele counts for case/control groups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hln-c8YuVxAk"
   },
   "source": [
    "### Now that we've assigned each call set to either the case or the control group, we can tally up the counts of reference and alternate alleles within each of our assigned case/control groups, for each variant position, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "IaBec5YoVxAk"
   },
   "outputs": [],
   "source": [
    "%%sql --module grouped_counts\n",
    "\n",
    "SELECT\n",
    "    reference_name,\n",
    "    start,\n",
    "    end,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    vt,\n",
    "    SUM(ref_count + alt_count) AS allele_count,\n",
    "    SUM(ref_count) AS ref_count,\n",
    "    SUM(alt_count) AS alt_count,\n",
    "    SUM(IF(TRUE = is_case, INTEGER(ref_count + alt_count), 0)) AS case_count,\n",
    "    SUM(IF(FALSE = is_case, INTEGER(ref_count + alt_count), 0)) AS control_count,\n",
    "    SUM(IF(TRUE = is_case, ref_count, 0)) AS case_ref_count,\n",
    "    SUM(IF(TRUE = is_case, alt_count, 0)) AS case_alt_count,\n",
    "    SUM(IF(FALSE = is_case, ref_count, 0)) AS control_ref_count,\n",
    "    SUM(IF(FALSE = is_case, alt_count, 0)) AS control_alt_count,\n",
    "FROM $snps\n",
    "GROUP BY\n",
    "    reference_name,\n",
    "    start,\n",
    "    end,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "jGf5JcijVxAq"
   },
   "outputs": [],
   "source": [
    "bq.Query(grouped_counts,\n",
    "         snps=snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vCSyha3bVxAs"
   },
   "source": [
    "### Quantify the statistical significance at each variant positions\n",
    "\n",
    "### We can quantify the statistical significance of each variant position using the Chi-squared test. Furthermore, we can restrict our result set to only statistically significant variant positions for this experiment by ranking each position by its statistical signficance (decreasing) and thresholding the results for significance at p <= 5e-8 (chi-squared score >= 29.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "27vbnpLlVxAt"
   },
   "outputs": [],
   "source": [
    "%%sql --module results\n",
    "\n",
    "SELECT\n",
    "  reference_name,\n",
    "  start,\n",
    "  end,\n",
    "  reference_bases,\n",
    "  alternate_bases,\n",
    "  vt,\n",
    "  case_count,\n",
    "  control_count,\n",
    "  allele_count,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "  case_ref_count,\n",
    "  case_alt_count,\n",
    "  control_ref_count,\n",
    "  control_alt_count,\n",
    "\n",
    "\n",
    "  ROUND(\n",
    "    POW(ABS(case_ref_count - (ref_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_ref_count - (ref_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*control_count) +\n",
    "    POW(ABS(case_alt_count - (alt_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_alt_count - (alt_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*control_count),\n",
    "    3) AS chi_squared_score\n",
    "FROM $grouped_counts\n",
    "WHERE\n",
    "  # For chi-squared, expected counts must be at least 5 for each group\n",
    "  (ref_count/allele_count)*case_count >= 5.0\n",
    "  AND (ref_count/allele_count)*control_count >= 5.0\n",
    "  AND (alt_count/allele_count)*case_count >= 5.0\n",
    "  AND (alt_count/allele_count)*control_count >= 5.0\n",
    "HAVING\n",
    "  # Chi-squared critical value for df=1, p-value=5*10^-8 is 29.71679\n",
    "  chi_squared_score >= 29.71679\n",
    "ORDER BY\n",
    "  chi_squared_score DESC,\n",
    "  allele_count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "OVphnZ4AVxAu"
   },
   "outputs": [],
   "source": [
    "bq.Query(results,\n",
    "         grouped_counts=grouped_counts,\n",
    "         snps=snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Iwm6OgzIVxAx"
   },
   "source": [
    "### the positions deemed significant do in fact have significantly different case/control counts for the alternate/reference bases.\n",
    "\n",
    "### Computing Chi-squared statistics in BigQuery vs Python\n",
    "\n",
    "### Let's compare these BigQuery-computed Chi-squared scores to ones calculated via Python's statistical packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "xDKXshK9VxAx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(np.array([ \n",
    "    [220, 352], # case \n",
    "    [1593, 19]  # control\n",
    "]))\n",
    "\n",
    "print 'Python Chi-sq score = %.3f' % chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "chOFkpGDVxAz"
   },
   "source": [
    "### We can see that for the computation in Python the value matches 1086.505 from BigQuery. ( repeat this in R and see that result values match)\n",
    "\n",
    "### Analyzing the GWAS results\n",
    "\n",
    "### First, how many statistically significant variant positions did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "39DkYS_9VxA0"
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT COUNT(*) AS num_significant_snps\n",
    "FROM $results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1oniVY--VxA2"
   },
   "source": [
    "### We now have a dataset that is sufficiently small to fit into memory on our instance, so let's pull the top 1000 SNP positions locally.\n",
    "\n",
    "### Since we only need a subset of the columns, we can project our data first to remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "udvkpfd0VxA3"
   },
   "outputs": [],
   "source": [
    "%%sql --module sig_snps_dataset\n",
    "SELECT * FROM (\n",
    "  SELECT\n",
    "    reference_name,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    chi_squared_score\n",
    "  FROM $results\n",
    "  LIMIT 1000\n",
    ")\n",
    "ORDER BY start asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "WNCNcYTXVxA7"
   },
   "outputs": [],
   "source": [
    "sig_snps = bq.Query(sig_snps_dataset, \n",
    "                    results=results,\n",
    "                    grouped_counts=grouped_counts,\n",
    "                    snps=snps,\n",
    "                    exp_groups=exp_groups,\n",
    "                    allele_counts=allele_counts,\n",
    "                    sample_info_table=sample_info_table,\n",
    "                    variants_table=variants_table).to_dataframe()\n",
    "\n",
    "sig_snps[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lmH3Uw3lVxBD"
   },
   "source": [
    "### Let's visualize the distribution of significant SNPs along the length of the chromosome.\n",
    "\n",
    "### The y-value of the charts indicates the Chi-squared score: larger values are more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "2qGI2-CtVxBD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#g = sns.distplot(sig_snps['start'], rug=False, hist=False, kde_kws=dict(bw=0.1))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps['start'], sig_snps['chi_squared_score'], alpha=0.3, c='red')\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5apUPZOuVxBG"
   },
   "source": [
    "### ok, now Let's zoom in on one region that contains a large number of very significant (high Chi Square Scores)  SNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "nZnP9tezVxBH"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps['start'], sig_snps['chi_squared_score'], alpha=0.5, c='red')\n",
    "ax.set_xlim([3.3e7, 3.5e7])\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8NafJ0uQVxBN"
   },
   "source": [
    "### Further exploration of statistically significant variant positions:\n",
    "\n",
    "### We can take our analysis further by mapping selected variant positions back to the chromosome and visualizing call sets and reads. Let's retrieve 'The TOP ONE' SNP identified when ranked by the Chi-squared score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KQR2JRMQVxBO"
   },
   "outputs": [],
   "source": [
    "%%sql --module top_snp\n",
    "SELECT start\n",
    "FROM $sig_snps_dataset\n",
    "ORDER BY chi_squared_score desc\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "YqA2bXmkVxBT"
   },
   "outputs": [],
   "source": [
    "bq.Query(top_snp,\n",
    "         sig_snps_dataset=sig_snps_dataset,\n",
    "         results=results,\n",
    "         grouped_counts=grouped_counts,\n",
    "         snps=snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Ql7wPawLVxBX"
   },
   "source": [
    "### That is IT. The most meaninful SNP variant beteween my control (read HEALTHY) group and my test (read SICK) group and this is 'the most likely culprit' (again, in reality is more complex but for the sake of the demo let's say this is our Agatha Christie moment and we pinpointed THE resonsible variant)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vtbY45YaVxBY"
   },
   "source": [
    "### Now just to show  how the genome Browser looks let's the grab an arbitrary set of 10 callset IDs for rendering in the genome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hr8qsrU3VxBZ"
   },
   "outputs": [],
   "source": [
    "%%sql --module callset_ids\n",
    "SELECT * FROM (\n",
    "  SELECT call.call_set_id AS callset_id\n",
    "  FROM $variants_table\n",
    "  GROUP BY callset_id)\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "WD32NdodVxBb"
   },
   "outputs": [],
   "source": [
    "callsets_df = bq.Query(callset_ids, variants_table=variants_table).to_dataframe()\n",
    "callsets = list(callsets_df['callset_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hmZG0sT3VxBe"
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "def gabrowse(dataset, reference_name, start_position, callset_ids):\n",
    "    callsets_query_params = ''.join('&callsetId=%s&cBackend=GOOGLE' % callset_id for callset_id in callset_ids)\n",
    "    url = ('https://gabrowse.appspot.com/#=&backend=GOOGLE&location=12%3A'\n",
    "         + str(start_position)\n",
    "         + callsets_query_params)\n",
    "    return HTML('<iframe src=\"%s\" width=1024 height=800></iframe>' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "XY6rFkK_VxBi"
   },
   "source": [
    "### Now we can render the call sets and reads for the selected SNP position by embedding the GABrowse application directly in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "9kkfcIiVVxBi"
   },
   "outputs": [],
   "source": [
    "gabrowse('1000genomes', '12', 110571373, callsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "yqpYwDzhVxBk"
   },
   "source": [
    "### TO SUMMARIZE IT ALL: \n",
    "### FIRST: this notebook illustrated how to run TWO Secondary genomic analysis pipeline on GCP (VERILY + GATK)\n",
    "### SECOND: how to conduct an entire GWAS experiment (which, for non-GCP users,  requires piping together 4 to 5 different tools and scripting the conections between the tools and the various languages et.c). \n",
    "### We accomplished all theee tasks from the same place (DATALAB) leaveraging many resources in the GCP Echosystem (GCS, GCE, GKE, BQ etc.) using publicly available VERY ALRGE variant data stored in BQ tables, and query it a AMAZING speed. Lastly after the filtering was done we imported a local copy of the most \"clinically meaningful results\" and visualized them with Python libraries. All from within one tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4csBRGOkVxBl"
   },
   "source": [
    "![](http://www.broadinstitute.org/gatk/img/GATK_BPP_white.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "YsGJc14RVxBl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "q_dAy8MjVw_f"
   ],
   "default_view": {},
   "name": "GATK_DV_Pipelines.ipynb",
   "provenance": [
    {
     "file_id": "1Lr7CSrnu0F6pu090EbZVwtDXtmf3mNPD",
     "timestamp": 1519137283728
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
